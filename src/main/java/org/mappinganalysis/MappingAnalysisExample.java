package org.mappinganalysis;import com.google.common.collect.Maps;import org.apache.commons.cli.*;import org.apache.flink.api.common.JobExecutionResult;import org.apache.flink.api.common.ProgramDescription;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.FlatJoinFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Triplet;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.apache.log4j.Logger;import org.mappinganalysis.graph.ClusterComputation;import org.mappinganalysis.graph.FlinkConnectedComponents;import org.mappinganalysis.io.JDBCDataLoader;import org.mappinganalysis.model.ObjectMap;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.*;import org.mappinganalysis.utils.Stats;import org.mappinganalysis.utils.Utils;import java.util.List;import java.util.Map;import java.util.Set;/** * Mapping analysis example */public class MappingAnalysisExample implements ProgramDescription {  private static final Logger LOG = Logger.getLogger(MappingAnalysisExample.class);  private static final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  /**   * Command line options   */  private static final String OPTION_DELETE_LINKS_PREPROCESSING = "dlp";  private static final String OPTION_PRE_CLUSTER_FILTER = "pcf";  private static final String OPTION_ONLY_INITIAL_CLUSTER = "ncc";//  private static final String OPTION_REPRESENTATIVE_STRATEGY = "rs";  private static final String OPTION_DATA_SET_NAME = "ds";  private static final String OPTION_WRITE_STATS = "ws";  private static Options OPTIONS;  static {    OPTIONS = new Options();    OPTIONS.addOption(OPTION_DATA_SET_NAME, "dataset-name", true, // done        "Choose one of the datasets [" + Utils.CMD_GEO + " (default), " + Utils.CMD_LL + "].");    OPTIONS.addOption(OPTION_DELETE_LINKS_PREPROCESSING, "link-sanity-1-n-strategy", false, // done        "Use delete 1:n strategy for initial link check (default: false).");    OPTIONS.addOption(OPTION_PRE_CLUSTER_FILTER, "pre-filter-strategy", true, // done        "Specify preprocessing filter strategy for entity properties ["            + Utils.CMD_COMBINED + " (default), geo, label, type]");    OPTIONS.addOption(OPTION_ONLY_INITIAL_CLUSTER, "only-initial-cluster", false, // done        "Don't compute final clusters, stop after preprocessing (default: false).");//    OPTIONS.addOption(OPTION_REPRESENTATIVE_STRATEGY, "representative-strategy",//        true, "Set strategy to determine cluster representative (currently only best datasource (default))");    OPTIONS.addOption(OPTION_WRITE_STATS, "write-stats", false,        "Write statistics to output (default: false)."); // done  }  public static void main(String[] args) throws Exception {    CommandLine cmd = parseArguments(args);    if (cmd == null) {      return;    }    String dataset;    final String optionDataset = cmd.getOptionValue(OPTION_DATA_SET_NAME);    if (optionDataset != null && optionDataset.equals(Utils.CMD_LL)) {      dataset = Utils.LL_FULL_NAME;    } else {      dataset = Utils.GEO_FULL_NAME;    }    final boolean isLinkFilterActive = cmd.hasOption(OPTION_DELETE_LINKS_PREPROCESSING);    final String preFilterStrategy = cmd.getOptionValue(OPTION_PRE_CLUSTER_FILTER, Utils.CMD_COMBINED);    final boolean stopAfterInitialClustering = cmd.hasOption(OPTION_ONLY_INITIAL_CLUSTER);    final boolean printStats = cmd.hasOption(OPTION_WRITE_STATS);    /**     * 0. Get data     */    Graph<Long, ObjectMap, NullValue> graph = getInputGraph(dataset);    /**     * 1. PREPROCESSING     */    graph = Preprocessing.applyLinkFilterStrategy(graph, env, isLinkFilterActive);    graph = Preprocessing.applyTypePreprocessing(graph, env);    final DataSet<Vertex<Long, ObjectMap>> baseVertices = graph.getVertices();    /**     * (3.) INITIAL CLUSTERING     * - connected components     */    final DataSet<Tuple2<Long, Long>> tmpEdges = graph.getEdges().project(0, 1);    final DataSet<Long> tmpVertices = baseVertices.map(new CcVerticesCreator());    FlinkConnectedComponents connectedComponents = new FlinkConnectedComponents(env);    final DataSet<Tuple2<Long, Long>> tmpComponents = connectedComponents.compute(tmpVertices, tmpEdges, 1000);    // update all vertices with ccId    final DataSet<Vertex<Long, ObjectMap>> ccResultVertices = baseVertices        .join(tmpComponents)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    /**     * (2.) INITIAL MATCHING     * - apply similarity functions, similarities are added as edge value and merged     * (if more than one similarity)     */    // compute all edges + sim vals    final DataSet<Edge<Long, NullValue>> allEdges        = ClusterComputation.computeComponentEdges(ccResultVertices);    final Graph<Long, ObjectMap, NullValue> allGraph = Graph        .fromDataSet(baseVertices, allEdges, ExecutionEnvironment.getExecutionEnvironment());    final DataSet<Triplet<Long, ObjectMap, ObjectMap>> accumulatedSimValues        = computeSimilarities(allGraph.getTriplets(), preFilterStrategy);    final DataSet<Tuple2<Long, Long>> ccEdges = accumulatedSimValues.project(0, 1);    // 2. time cc    final DataSet<Tuple2<Long, Long>> components = connectedComponents        .compute(tmpVertices, ccEdges, 1000);    final DataSet<Vertex<Long, ObjectMap>> verticesWithComponentId = baseVertices        .join(components)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    // update triplets with edge props//    DataSet<Edge<Long, Map<String, Object>>> joinedEdges = graph//        .getEdges()//        .join(accumulatedSimValues)//        .where(0, 1).equalTo(0, 1)//        .with(new CcResultEdgesJoin());    if (stopAfterInitialClustering) {      if (printStats) {//        graph.getVertexIds().collect(); // how to get rid of this collect job TODO        Stats.countPrintResourcesPerCc(components);        JobExecutionResult vertexJobExecResult = env.getLastJobExecutionResult();//        JobExecutionResult edgeJobExecResult = env.getLastJobExecutionResult();        LOG.info("Number of incorrect links: " + vertexJobExecResult.getAccumulatorResult(Utils.LINK_FILTER_ACCUMULATOR));        LOG.info("Edges imported: " + vertexJobExecResult.getAccumulatorResult(Utils.EDGE_COUNT_ACCUMULATOR));        LOG.info("Properties imported: " + vertexJobExecResult.getAccumulatorResult(Utils.PROP_COUNT_ACCUMULATOR));        LOG.info("Vertices imported: " + vertexJobExecResult.getAccumulatorResult(Utils.VERTEX_COUNT_ACCUMULATOR));        Map<String, Long> typeStats = Maps.newHashMap();        List<String> typesList = vertexJobExecResult.getAccumulatorResult(Utils.TYPES_COUNT_ACCUMULATOR);        for (String s : typesList) {          if (typeStats.containsKey(s)) {            typeStats.put(s, typeStats.get(s) + 1L);          } else {            typeStats.put(s, 1L);          }        }        LOG.info("### --- Type counts parsed to internal type in preprocessing:");        for (Map.Entry<String, Long> entry : typeStats.entrySet()) {          LOG.info(entry.getKey() + ": " + entry.getValue());        }        LOG.info("### --- Accumulators + values:");        for (Map.Entry<String, Object> entry : vertexJobExecResult.getAllAccumulatorResults().entrySet()) {          if (entry.getKey().length() != 32 && !entry.getKey().equals(Utils.TYPES_COUNT_ACCUMULATOR)) {            LOG.info(entry.getKey() + ": " + entry.getValue());          }        }      }      return;    }    /**     * 4. Determination of cluster representative     * - currently: entity from best "data source" (GeoNames > DBpedia > others)     */    final DataSet<Vertex<Long, ObjectMap>> mergedCluster = verticesWithComponentId        .groupBy(new CcIdKeySelector())        .reduceGroup(new BestDataSourceAllLabelsGroupReduceFunction());    // TODO print at end of program until #15 is fixed    if (printStats) {      Stats.countPrintResourcesPerCc(components);      Stats.printLabelsForMergedClusters(mergedCluster);    }    /**     * 5. Cluster Refinement     */    mergedCluster.filter(new FilterFunction<Vertex<Long, ObjectMap>>() {      @Override      public boolean filter(Vertex<Long, ObjectMap> vertex) throws Exception {         return !(vertex.getValue().get(Utils.CL_VERTICES) instanceof Set);      }    });    DataSet<Edge<Long, Double>> edgesCrossedClusters = mergedCluster        .cross(mergedCluster)        .with(new ClusterEdgeCreationCrossFunction())        .filter(new FilterFunction<Edge<Long, Double>>() {          @Override          public boolean filter(Edge<Long, Double> edge) throws Exception {            return edge.getValue() > 0.7;          }        });    if (printStats) {      edgesCrossedClusters.print();      System.out.println(edgesCrossedClusters.count());    }  }  public static DataSet<Triplet<Long, ObjectMap, ObjectMap>>  computeSimilarities(DataSet<Vertex<Long, ObjectMap>> vertices,                      DataSet<Edge<Long, NullValue>> edges,                      String filter) {    DataSet<Triplet<Long, ObjectMap, NullValue>> triplets = Graph        .fromDataSet(vertices, edges, env)        .getTriplets();    return computeSimilarities(triplets, filter);  }  public static DataSet<Triplet<Long, ObjectMap, ObjectMap>>  computeSimilarities(DataSet<Triplet<Long, ObjectMap, NullValue>> triplets, String filter) {    switch (filter) {      case "geo":        return basicGeoSimilarity(triplets);      case "label":        return basicTrigramSimilarity(triplets);      case "type":        return basicTypeSimilarity(triplets);      default:        return joinDifferentSimilarityValues(basicGeoSimilarity(triplets),            basicTrigramSimilarity(triplets),            basicTypeSimilarity(triplets));    }  }  @SafeVarargs  private static DataSet<Triplet<Long, ObjectMap, ObjectMap>> joinDifferentSimilarityValues(      DataSet<Triplet<Long, ObjectMap, ObjectMap>>... tripletDataSet) {    DataSet<Triplet<Long, ObjectMap, ObjectMap>> triplets = null;    boolean first = false;    for (DataSet<Triplet<Long, ObjectMap, ObjectMap>> dataSet : tripletDataSet) {      if (!first) {        triplets = dataSet;        first = true;      } else {        triplets = triplets            .fullOuterJoin(dataSet)            .where(0, 1)            .equalTo(0, 1)            .with(new JoinSimilarityValueFunction());      }    }    return triplets;  }  private static DataSet<Triplet<Long, ObjectMap, ObjectMap>> basicTypeSimilarity(      DataSet<Triplet<Long, ObjectMap, NullValue>> baseTriplets) {    return baseTriplets        .map(new TypeSimilarityMapper())        .filter(new TypeFilter());  }  private static DataSet<Triplet<Long, ObjectMap, ObjectMap>> basicTrigramSimilarity(      DataSet<Triplet<Long, ObjectMap, NullValue>> baseTriplets) {    return baseTriplets        .map(new TrigramSimilarityMapper())        .filter(new TrigramSimilarityFilter());  }  private static DataSet<Triplet<Long, ObjectMap, ObjectMap>> basicGeoSimilarity(      DataSet<Triplet<Long, ObjectMap, NullValue>> baseTriplets) {    return baseTriplets        .filter(new EmptyGeoCodeFilter())        .map(new GeoCodeSimFunction())        .filter(new GeoCodeThreshold());  }  /**   * Create the input graph for further analysis,   * restrict to edges where source and target are in vertices set.   * @return graph with vertices and edges.   * @throws Exception   * @param fullDbString complete server+port+db string   */  public static Graph<Long, ObjectMap, NullValue> getInputGraph(String fullDbString)      throws Exception {    JDBCDataLoader loader = new JDBCDataLoader(env);    DataSet<Vertex<Long, ObjectMap>> vertices = loader.getVertices(fullDbString);    // restrict edges to these where source and target are vertices    DataSet<Edge<Long, NullValue>> edges = loader.getEdges(fullDbString)        .leftOuterJoin(vertices)        .where(0).equalTo(0)        .with(new EdgeRestrictFlatJoinFunction())        .leftOuterJoin(vertices)        .where(1).equalTo(0)        .with(new EdgeRestrictFlatJoinFunction());    // delete vertices without any edges due to restriction    DataSet<Vertex<Long, ObjectMap>> left = vertices        .leftOuterJoin(edges)        .where(0).equalTo(0)        .with(new VertexRestrictFlatJoinFunction()).distinct(0);    DataSet<Vertex<Long, ObjectMap>> finalVertices = vertices        .leftOuterJoin(edges)        .where(0).equalTo(1)        .with(new VertexRestrictFlatJoinFunction()).distinct(0)        .union(left);    return Graph.fromDataSet(finalVertices, edges, env);  }  /**   * Parses the program arguments or returns help if args are empty.   *   * @param args program arguments   * @return command line which can be used in the program   */  private static CommandLine parseArguments(String[] args) throws ParseException {    if (args.length == 0) {      HelpFormatter formatter = new HelpFormatter();      formatter.printHelp(MappingAnalysisExample.class.getName(), OPTIONS, true);      return null;    }    CommandLineParser parser = new BasicParser();    return parser.parse(OPTIONS, args);  }  @Override  public String getDescription() {    return MappingAnalysisExample.class.getName();  }  private static class EdgeRestrictFlatJoinFunction implements FlatJoinFunction<Edge<Long, NullValue>,      Vertex<Long, ObjectMap>, Edge<Long, NullValue>> {    @Override    public void join(Edge<Long, NullValue> edge, Vertex<Long, ObjectMap> vertex,                     Collector<Edge<Long, NullValue>> collector) throws Exception {      if (vertex != null) {        collector.collect(edge);      }    }  }  private static class VertexRestrictFlatJoinFunction implements FlatJoinFunction<Vertex<Long, ObjectMap>,      Edge<Long, NullValue>, Vertex<Long, ObjectMap>> {    @Override    public void join(Vertex<Long, ObjectMap> vertex, Edge<Long, NullValue> edge,                     Collector<Vertex<Long, ObjectMap>> collector) throws Exception {      if (edge != null) {        collector.collect(vertex);      }    }  }}