package org.mappinganalysis;import com.google.common.collect.Lists;import com.google.common.collect.Sets;import org.apache.commons.cli.*;import org.apache.flink.api.common.ProgramDescription;import org.apache.flink.api.common.accumulators.LongCounter;import org.apache.flink.api.common.functions.*;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.LocalEnvironment;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.operators.GroupReduceOperator;import org.apache.flink.api.java.operators.IterativeDataSet;import org.apache.flink.api.java.tuple.Tuple1;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.configuration.Configuration;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Triplet;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.apache.log4j.Logger;import org.mappinganalysis.model.ObjectMap;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.CcIdKeySelector;import org.mappinganalysis.model.functions.HashCcIdKeySelector;import org.mappinganalysis.model.functions.preprocessing.AddShadingTypeMapFunction;import org.mappinganalysis.model.functions.preprocessing.CcIdAndCompTypeKeySelector;import org.mappinganalysis.model.functions.preprocessing.GenerateHashCcIdGroupReduceFunction;import org.mappinganalysis.model.functions.representative.*;import org.mappinganalysis.model.functions.simcomputation.AggSimValueTripletMapFunction;import org.mappinganalysis.model.functions.simcomputation.SimCompUtility;import org.mappinganalysis.model.functions.simsort.SimSort;import org.mappinganalysis.model.functions.stats.FrequencyMapByFunction;import org.mappinganalysis.model.functions.typegroupby.TypeGroupBy;import org.mappinganalysis.utils.Stats;import org.mappinganalysis.utils.Utils;import java.util.*;/** * Mapping analysis example */public class MappingAnalysisExample implements ProgramDescription {  private static final Logger LOG = Logger.getLogger(MappingAnalysisExample.class);  private static ExecutionEnvironment env;// = ExecutionEnvironment.getExecutionEnvironment();  /**   * Command line options   */  private static final String OPTION_LINK_FILTER_PREPROCESSING = "lfp";  private static final String OPTION_PRE_CLUSTER_FILTER = "pcf";  private static final String OPTION_ONLY_INITIAL_CLUSTER = "oic";//  private static final String OPTION_REPRESENTATIVE_STRATEGY = "rs";  private static final String OPTION_DATA_SET_NAME = "ds";  private static final String OPTION_WRITE_STATS = "ws";  private static final String OPTION_CLUSTER_STATS = "cs";  private static final String OPTION_IGNORE_MISSING_PROPERTIES = "imp";  private static final String OPTION_PROCESSING_MODE = "pm";  private static final String OPTION_TYPE_MISS_MATCH_CORRECTION = "tmmc";  private static boolean IS_TYPE_MISS_MATCH_CORRECTION_ACTIVE;  private static boolean STOP_AFTER_INITIAL_CLUSTERING;  private static String PROCESSING_MODE;  private static List<Long> CLUSTER_STATS;  private static Options OPTIONS;  static {    OPTIONS = new Options();    // general    OPTIONS.addOption(OPTION_DATA_SET_NAME, "dataset-name", true,        "Choose one of the datasets [" + Utils.CMD_GEO + " (default), " + Utils.CMD_LL + "].");    OPTIONS.addOption(OPTION_PROCESSING_MODE, "processing-mode", true,        "Choose the processing mode [SimSort + TypeGroupBy (default), simSortOnly].");    OPTIONS.addOption(OPTION_IGNORE_MISSING_PROPERTIES, "ignore-missing-properties", false,        "Do not penalize missing properties on resources in similarity computation process (default: false).");    OPTIONS.addOption(OPTION_ONLY_INITIAL_CLUSTER, "only-initial-cluster", false,        "Don't compute final clusters, stop after preprocessing (default: false).");    // Preprocessing    OPTIONS.addOption(OPTION_LINK_FILTER_PREPROCESSING, "link-filter-preprocessing", false,        "Exclude edges where vertex has several target vertices having equal dataset ontology (default: false).");    OPTIONS.addOption(OPTION_TYPE_MISS_MATCH_CORRECTION, "type-miss-match-correction", false,        "Exclude edges where directly connected source and target vertices have different type property values. " +            "(default: false).");    // todo to be changed    OPTIONS.addOption(OPTION_PRE_CLUSTER_FILTER, "pre-cluster-filter", true,        "Specify preprocessing filter strategy for entity properties ["            + Utils.DEFAULT_VALUE + " (combined), geo, label, type]");//    OPTIONS.addOption(OPTION_REPRESENTATIVE_STRATEGY, "representative-strategy",//        true, "Set strategy to determine cluster representative (currently only best datasource (default))");    OPTIONS.addOption(OPTION_WRITE_STATS, "write-stats", false,        "Write statistics to output (default: false).");    Option clusterStats = new Option(OPTION_CLUSTER_STATS, "cluster-stats", true,        "Be more verbose while processing specified cluster ids.");    clusterStats.setArgs(Option.UNLIMITED_VALUES);    OPTIONS.addOption(clusterStats);  }  /**   * main program   * @param args cmd args   * @throws Exception   */  public static void main(String[] args) throws Exception {//    Configuration conf = new Configuration();//    conf.setLong("taskmanager.network.numberOfBuffers", 65536L);//    env =  new LocalEnvironment(conf);//    env.setParallelism(6);    env = ExecutionEnvironment.getExecutionEnvironment();    CommandLine cmd = parseArguments(args);    if (cmd == null) {      return;    }    String dataset;    final String optionDataset = cmd.getOptionValue(OPTION_DATA_SET_NAME);    if (optionDataset != null && optionDataset.equals(Utils.CMD_LL)) {      dataset = Utils.LL_FULL_NAME;    } else {      dataset = Utils.LL_FULL_NAME;//GEO_FULL_NAME;    }    Utils.IS_LINK_FILTER_ACTIVE = cmd.hasOption(OPTION_LINK_FILTER_PREPROCESSING);    IS_TYPE_MISS_MATCH_CORRECTION_ACTIVE = cmd.hasOption(OPTION_TYPE_MISS_MATCH_CORRECTION);    PROCESSING_MODE = cmd.getOptionValue(OPTION_PROCESSING_MODE, Utils.DEFAULT_VALUE);    Utils.IGNORE_MISSING_PROPERTIES = cmd.hasOption(OPTION_IGNORE_MISSING_PROPERTIES);    Utils.PRE_CLUSTER_STRATEGY = cmd.getOptionValue(OPTION_PRE_CLUSTER_FILTER, Utils.DEFAULT_VALUE);    STOP_AFTER_INITIAL_CLUSTERING = cmd.hasOption(OPTION_ONLY_INITIAL_CLUSTER);    Utils.PRINT_STATS = cmd.hasOption(OPTION_WRITE_STATS);    String[] clusterStats = cmd.getOptionValues(OPTION_CLUSTER_STATS);    if (clusterStats != null) {      CLUSTER_STATS = Utils.convertWsSparatedString(clusterStats);    }    String ds = dataset.equals(Utils.LL_FULL_NAME) ? "linklion" : "geo";    LOG.info("");    LOG.info("[0] GET DATASET " + ds);    LOG.info("###############");    final Graph<Long, ObjectMap, NullValue> graph = Preprocessing.getInputGraph(dataset, env);    execute(graph);  }  /**   * Mapping analysis computation   * @param preprocGraph input graph   * @throws Exception   */  private static void execute(Graph<Long, ObjectMap, NullValue> preprocGraph) throws Exception {    LOG.info("");    LOG.info("[1] PREPROCESSING");    LOG.info("#################");//    ArrayList<Long> clusterList = Lists.newArrayList(1458L);//, 2913L);//, 4966L, 5678L);    final ArrayList<Long> vertexList = Lists.newArrayList(3101L, 541L, 6703L, 3872L);    preprocGraph = Preprocessing.applyTypeToInternalTypeMapping(preprocGraph, env);    preprocGraph = Preprocessing.applyLinkFilterStrategy(preprocGraph, env, Utils.IS_LINK_FILTER_ACTIVE);//    preprocGraph = Preprocessing.applyTypeMissMatchCorrection(preprocGraph, IS_TYPE_MISS_MATCH_CORRECTION_ACTIVE);    preprocGraph = Preprocessing.addCcIdsToGraph(preprocGraph);//    Stats.writeVerticesToLog(preprocGraph.getVertices(), vertexList);//    Stats.writeVerticesToLog(preprocGraph.getVertices(), vertexList);    DataSet<Vertex<Long, ObjectMap>> vertices = preprocGraph.getVertices()        .map(new AddShadingTypeMapFunction())        .groupBy(new CcIdAndCompTypeKeySelector())        .reduceGroup(new GenerateHashCcIdGroupReduceFunction());    KeySelector<Vertex<Long, ObjectMap>, Long> simSortKeySelector = new CcIdKeySelector();    LOG.info("");    LOG.info("[2] INITIAL MATCHING");    // apply similarity functions, similarities are added as edge value and merged (if more than one similarity)    LOG.info("####################");    DataSet<Edge<Long, ObjectMap>> edges = SimCompUtility.computeEdgeSimFromGraph(preprocGraph);    Graph<Long, ObjectMap, ObjectMap> graph = Graph.fromDataSet(vertices, edges, env);    LOG.info("");    LOG.info("[3] INITIAL CLUSTERING");    LOG.info("######################");    /*     * TypeGroupBy: internally compType is used, afterwards typeIntern is used again.     */    if (PROCESSING_MODE.equals(Utils.DEFAULT_VALUE)) {      graph = new TypeGroupBy().execute(graph, 1000);      simSortKeySelector = new HashCcIdKeySelector();//      LOG.info("##################### After TypeGroupBy");//      Stats.writeVerticesToLog(graph.getVertices(), vertexList);    }    /*     * SimSort     */    graph = SimSort.prepare(graph, simSortKeySelector, env);    final double minClusterSim = 0.7;    graph = SimSort.execute(graph, 1000, minClusterSim);    graph = SimSort.excludeLowSimVertices(graph, env);//    LOG.info("######################## After SimSort");//    Stats.writeVerticesToLog(graph.getVertices(), vertexList);      if (Utils.PRINT_STATS) {        LOG.info("");        LOG.info("### Statistics: ");//        Stats.printAccumulatorValues(env, graph, simSortKeySelector);//        Stats.printComponentSizeAndCount(graph.getVertices());        //TODO//        Stats.countPrintGeoPointsPerOntology(preprocGraph);//        printEdgesSimValueBelowThreshold(allEdgesGraph, accumulatedSimValues);      }    /**     * 4. Determination of cluster representative     */    DataSet<Vertex<Long, ObjectMap>> representativeVertices = graph.getVertices()        .groupBy(new HashCcIdKeySelector())        .reduceGroup(new MajorityPropertiesGroupReduceFunction());//    LOG.info("representativeVertices 1. count: " + representativeVertices.count());    // TODO optional stats//    DataSet<Vertex<Long, ObjectMap>> reprFilter = representativeVertices.filter(new EvalVerticesCheckFilter(vertexList));//    for (Vertex<Long, ObjectMap> vertex : reprFilter.collect()) {//      LOG.info("representative: " + vertex);//    }//    DataSet<Tuple2<String, Long>> statsSumTypeCounts = representativeVertices//        .map(new MapVertexToPropertyStringFunction(Utils.TYPE_INTERN))//        .groupBy(0).sum(1);//    LOG.info("[3] ### Type property counts after creation of representative: ");//    for (Tuple2<String, Long> tuple2 : statsSumTypeCounts.collect()) {//      LOG.info("[3] " + tuple2.f0 + " count: " + tuple2.f1);//    }    /**     * 5. Cluster Refinement     */    DataSet<Triplet<Long, ObjectMap, NullValue>> sortedOutSimSortTriplets = representativeVertices        .filter(new OldHashCcFilterFunction())        .groupBy(new OldHashCcKeySelector())        .reduceGroup(new TripletCreateGroupReduceFunction());    representativeVertices = integrateMergedVertices(representativeVertices, sortedOutSimSortTriplets, minClusterSim);//    LOG.info("representativeVertices 2. count: " + representativeVertices.count());//    representativeVertices = refine(representativeVertices);//    LOG.info("representativeVertices 3. count: " + representativeVertices.count());    DataSet<Tuple2<Long, Long>> finalClusterSizes = representativeVertices        .map(new MapFunction<Vertex<Long, ObjectMap>, Tuple2<Long, Long>>() {          @Override          public Tuple2<Long, Long> map(Vertex<Long, ObjectMap> vertex) throws Exception {            return new Tuple2<>((long) vertex.getValue().getVerticesList().size(), 1L);          }        })        .groupBy(0).sum(1);    for (Tuple2<Long, Long> tuple2 : finalClusterSizes.collect()) {      LOG.info("final: " + tuple2);    }//    finalClusterSizes.writeAsText("finalClusterSizes.txt");//////    LOG.info(env.execute());    // todo LOG output//    LOG.info("[3] ### Final cluster sizes after refinement: ");//    for (Tuple2<Long, Long> tuple : finalClusterSizes.collect()) {//      LOG.info("[3] size, count: " + tuple);//    }////    // todo LOG output//    DataSet<Vertex<Long, ObjectMap>> lastFilter = unionVertices.filter(new EvalVerticesCheckFilter(vertexList));//    for (Vertex<Long, ObjectMap> vertex : lastFilter.collect()) {//      LOG.info("lastFilter: " + vertex);//    }////    JobExecutionResult jobExecResult = env.getLastJobExecutionResult();//    LOG.info("[3] ### Representatives created: "//        + jobExecResult.getAccumulatorResult(Utils.REPRESENTATIVE_ACCUMULATOR));//    LOG.info("[3] ### Clusters created in refinement step: "//        + jobExecResult.getAccumulatorResult(Utils.REFINEMENT_MERGE_ACCUMULATOR));//    LOG.info("[3] ### Excluded vertex counter: "//        + jobExecResult.getAccumulatorResult(Utils.EXCLUDE_VERTEX_ACCUMULATOR));//    newClusters.print();  }  /**   * @param inVertices input vertices dataset   * @return result   * @throws Exception   */  public static DataSet<Vertex<Long, ObjectMap>> refine(DataSet<Vertex<Long, ObjectMap>> inVertices) throws Exception {//    IterativeDataSet<Vertex<Long, ObjectMap>> loop = inVertices.iterate(4);//    loop.getMaxIterations()    DataSet<Integer> foo = env.fromElements(0, 1, 2, 3);    int maxClusterSize = 4;//    for (int leftSize = 1; leftSize < maxClusterSize; leftSize++) {      LOG.info("leftSize " + 1);      DataSet<Vertex<Long, ObjectMap>> left = inVertices          .filter(new ClusterSizeFilterFunction(1)).withBroadcastSet(foo, "counter");      DataSet<Vertex<Long, ObjectMap>> right = inVertices          .filter(new ClusterSizeFilterFunction(1, maxClusterSize)).withBroadcastSet(foo, "counter");      DataSet<Triplet<Long, ObjectMap, NullValue>> inTriplets = left.cross(right)          .with(new TripletCreateCrossFunction())          .filter(new EmptyTripletDeleteFilter());      // - similarity on intriplets + threshold      DataSet<Triplet<Long, ObjectMap, ObjectMap>> similarTriplets = SimCompUtility          .computeSimilarities(inTriplets, Utils.DEFAULT_VALUE)          .map(new AggSimValueTripletMapFunction(Utils.IGNORE_MISSING_PROPERTIES)).withForwardedFields("f0;f1;f2;f3")          .filter(new MinRequirementThresholdFilterFunction(0.7));      // - exclude duplicate ontology vertices      // - mark matches with more than 1 equal src/trg high similarity triplets      similarTriplets = similarTriplets.leftOuterJoin(extractExcludeTriplets(similarTriplets))          .where(0,1)          .equalTo(0,1)          .with(new FlatJoinFunction<Triplet<Long,ObjectMap,ObjectMap>, Tuple3<Long,Long, Long>,              Triplet<Long, ObjectMap, ObjectMap>>() {            @Override            public void join(Triplet<Long, ObjectMap, ObjectMap> triplet, Tuple3<Long, Long, Long> tuple,                             Collector<Triplet<Long, ObjectMap, ObjectMap>> collector) throws Exception {              if (tuple == null) {                collector.collect(triplet);              } else if (tuple.f2 != Long.MIN_VALUE) {                triplet.getSrcVertex().getValue().put(Utils.REFINE_ID, tuple.f2);                triplet.getTrgVertex().getValue().put(Utils.REFINE_ID, tuple.f2);//                LOG.info("BIG cluster: " + triplet);                collector.collect(triplet);              } else {//                LOG.info("sorted out: " + triplet);              }            }          });      // - create cluster for marked triplets, e.g. (1, 2), (1, 3) merged to cluster vertex with vertex list (1,2,3)      DataSet<Vertex<Long, ObjectMap>> partlyVertices = similarTriplets          .filter(new RefineIdFilterFunction())          .flatMap(new VertexExtractFlatMapFunction())          .groupBy(new RefineIdKeySelector())          .reduceGroup(new MajorityPropertiesGroupReduceFunction());      DataSet<Vertex<Long, ObjectMap>> newClusters = similarTriplets          .filter(new RefineIdExcludeFilterFunction())          .map(new SimilarClusterMergeMapFunction()); // REFINEMENT_MERGE_ACCUMULATOR - new cluster count      inVertices =  similarTriplets          .flatMap(new VertexExtractFlatMapFunction())          .<Tuple1<Long>>project(0)          .distinct()          .rightOuterJoin(inVertices)          .where(0).equalTo(0)          .with(new ExcludeVertexFlatJoinFunction())          .union(newClusters) // EXCLUDE_VERTEX_ACCUMULATOR counter          .union(partlyVertices);//      LOG.info("inVertives count: " + inVertices.count());//    }    return inVertices;  }  /**   * Exclude   * 1. tuples where duplicate ontologies are found   * 2. tuples where more than one match occures   */  private static DataSet<Tuple3<Long, Long, Long>> extractExcludeTriplets(      DataSet<Triplet<Long, ObjectMap, ObjectMap>> similarTriplets) throws Exception {    DataSet<Triplet<Long, ObjectMap, ObjectMap>> equalSourceVertex = getDuplicateTriplets(similarTriplets, 0);    // TODO ???//    if (LOG.isInfoEnabled())//    for (Triplet<Long, ObjectMap, ObjectMap> triplet : equalSourceVertex.collect()) {//      LOG.info("LLL equal triplet: " + triplet);//      break;//    }    DataSet<Triplet<Long, ObjectMap, ObjectMap>> equalTargetVertex = getDuplicateTriplets(similarTriplets, 1);    return excludeTuples(equalSourceVertex, 1)        .union(excludeTuples(equalTargetVertex, 0))        .distinct();  }  /**   * Exclude   * 1. tuples where duplicate ontologies are found   * 2. tuples where more than one match occures   * @param triplets input triplets   * @param column 0 - source, 1 - target   * @return tuples which should be excluded   */  private static DataSet<Tuple3<Long, Long, Long>> excludeTuples(DataSet<Triplet<Long, ObjectMap, ObjectMap>> triplets,                                                                 final int column) {    return triplets.groupBy(1 - column)        .reduceGroup(new GroupReduceFunction<Triplet<Long, ObjectMap, ObjectMap>, Tuple3<Long, Long, Long>>() {          @Override          public void reduce(Iterable<Triplet<Long, ObjectMap, ObjectMap>> triplets,                             Collector<Tuple3<Long, Long, Long>> collector) throws Exception {            Set<Tuple2<Long, Long>> tripletIds = Sets.newHashSet();            Long clusterRefineId = fillIdsCheckDuplicateOntology(triplets, tripletIds, column);            if (clusterRefineId == null) {//              LOG.info("Exclude all case" + tripletIds.toString());              for (Tuple2<Long, Long> tripletId : tripletIds) {                collector.collect(new Tuple3<>(tripletId.f0, tripletId.f1, Long.MIN_VALUE));              }            } else {//              LOG.info("Exclude none + enrich" + tripletIds.toString());              for (Tuple2<Long, Long> tripletId : tripletIds) {                collector.collect(new Tuple3<>(tripletId.f0, tripletId.f1, clusterRefineId));              }            }          }          /**           * quick'n'dirty fix haha later todo           */          private Long fillIdsCheckDuplicateOntology(Iterable<Triplet<Long, ObjectMap, ObjectMap>> triplets,                                                        Set<Tuple2<Long, Long>> tripletIds, int column) {            Set<String> ontologies = Sets.newHashSet();            Long minimumId = null;            for (Triplet<Long, ObjectMap, ObjectMap> triplet : triplets) {              Long srcId = triplet.getSrcVertex().getId();              Long trgId = triplet.getTrgVertex().getId();              tripletIds.add(new Tuple2<>(srcId, trgId));              Long tmpId = srcId < trgId ? srcId : trgId;              if (minimumId == null || minimumId > tmpId) {                minimumId = tmpId;              }              Set<String> tripletOnts;              if (column == 1) {                tripletOnts = (Set<String>) triplet.getTrgVertex().getValue().get(Utils.ONTOLOGIES);              } else {                tripletOnts = (Set<String>) triplet.getSrcVertex().getValue().get(Utils.ONTOLOGIES);              }              for (String ont : tripletOnts) {                if (!ontologies.add(ont)) {//                  LOG.info("problem duplicate ontology triplet: " + triplet);                  minimumId = null;                  break;                }              }            }            return minimumId;          }        });  }  /**   * Return triplet data for certain vertex id's.   * @param similarTriplets source triplets   * @param column search for vertex id in triplets source (0) or target (1)   * @return resulting triplets   */  private static DataSet<Triplet<Long, ObjectMap, ObjectMap>> getDuplicateTriplets(DataSet<Triplet<Long, ObjectMap,      ObjectMap>> similarTriplets, int column) {    return similarTriplets.<Tuple2<Long, Long>>project(0, 1).map(new FrequencyMapByFunction(column))        .groupBy(0)        .sum(1)        .filter(new FilterFunction<Tuple2<Long, Long>>() {          @Override          public boolean filter(Tuple2<Long, Long> tuple) throws Exception {            return tuple.f1 > 1;          }        })        .leftOuterJoin(similarTriplets)        .where(0)        .equalTo(column)        .with(new JoinFunction<Tuple2<Long, Long>, Triplet<Long, ObjectMap, ObjectMap>,            Triplet<Long, ObjectMap, ObjectMap>>() {          @Override          public Triplet<Long, ObjectMap, ObjectMap> join(Tuple2<Long, Long> tuple,              Triplet<Long, ObjectMap, ObjectMap> triplet) throws Exception {            return triplet;          }        });  }  private static DataSet<Vertex<Long, ObjectMap>> integrateMergedVertices(      DataSet<Vertex<Long, ObjectMap>> mergedClusterVertices,      DataSet<Triplet<Long, ObjectMap, NullValue>> sortedOutSimSortTriplets, double threshold) {    DataSet<Triplet<Long, ObjectMap, ObjectMap>> newReprBaseTriplets = SimCompUtility        .computeSimilarities(sortedOutSimSortTriplets, Utils.DEFAULT_VALUE)        .map(new AggSimValueTripletMapFunction(Utils.IGNORE_MISSING_PROPERTIES)).withForwardedFields("f0;f1;f2;f3")        .filter(new MinRequirementThresholdFilterFunction(threshold));    DataSet<Vertex<Long, ObjectMap>> newRepresentativeVertices = newReprBaseTriplets        .flatMap(new VertexExtractFlatMapFunction())        .groupBy(new OldHashCcKeySelector())        .reduceGroup(new MajorityPropertiesGroupReduceFunction());    return newReprBaseTriplets        .flatMap(new VertexExtractFlatMapFunction())        .<Tuple1<Long>>project(0)        .distinct()        .rightOuterJoin(mergedClusterVertices)        .where(0)        .equalTo(0)        .with(new ExcludeVertexFlatJoinFunction())        .union(newRepresentativeVertices);  }  /**   * Parses the program arguments or returns help if args are empty.   *   * @param args program arguments   * @return command line which can be used in the program   */  private static CommandLine parseArguments(String[] args) throws ParseException {    if (args.length == 0) {      HelpFormatter formatter = new HelpFormatter();      formatter.printHelp(MappingAnalysisExample.class.getName(), OPTIONS, true);      return null;    }    CommandLineParser parser = new BasicParser();    return parser.parse(OPTIONS, args);  }  @Override  public String getDescription() {    return MappingAnalysisExample.class.getName();  }  private static class ExcludeVertexFlatJoinFunction extends RichFlatJoinFunction<Tuple1<Long>,      Vertex<Long, ObjectMap>, Vertex<Long, ObjectMap>> {    private LongCounter excludeCounter = new LongCounter();    @Override    public void open(final Configuration parameters) throws Exception {      super.open(parameters);      getRuntimeContext().addAccumulator(Utils.EXCLUDE_VERTEX_ACCUMULATOR, excludeCounter);    }    @Override    public void join(Tuple1<Long> left, Vertex<Long, ObjectMap> right,                     Collector<Vertex<Long, ObjectMap>> collector) throws Exception {      if (left == null) {        excludeCounter.add(1L);        collector.collect(right);      }    }  }  private static class OldHashCcFilterFunction implements FilterFunction<Vertex<Long, ObjectMap>> {    @Override    public boolean filter(Vertex<Long, ObjectMap> vertex) throws Exception {      return vertex.getValue().containsKey(Utils.OLD_HASH_CC);    }  }  private static class OldHashCcKeySelector implements KeySelector<Vertex<Long, ObjectMap>, Long> {    @Override    public Long getKey(Vertex<Long, ObjectMap> vertex) throws Exception {      return (long) vertex.getValue().get(Utils.OLD_HASH_CC);    }  }  private static class VertexExtractFlatMapFunction implements FlatMapFunction<Triplet<Long, ObjectMap, ObjectMap>,      Vertex<Long, ObjectMap>> {    @Override    public void flatMap(Triplet<Long, ObjectMap, ObjectMap> triplet, Collector<Vertex<Long, ObjectMap>> collector)        throws Exception {      collector.collect(new Vertex<>(triplet.getSrcVertex().getId(), triplet.getSrcVertex().getValue()));      collector.collect(new Vertex<>(triplet.getTrgVertex().getId(), triplet.getTrgVertex().getValue()));    }  }  private static class TripletCreateGroupReduceFunction implements GroupReduceFunction<Vertex<Long, ObjectMap>,      Triplet<Long, ObjectMap, NullValue>> {    @Override    public void reduce(Iterable<Vertex<Long, ObjectMap>> vertices,                       Collector<Triplet<Long, ObjectMap, NullValue>> collector) throws Exception {      HashSet<Vertex<Long, ObjectMap>> rightSet = Sets.newHashSet(vertices);      HashSet<Vertex<Long, ObjectMap>> leftSet = Sets.newHashSet(rightSet);      for (Vertex<Long, ObjectMap> left : leftSet) {        for (Vertex<Long, ObjectMap> right : rightSet) {          if ((long) right.getId() != left.getId())            collector.collect(new Triplet<>(left.getId(), right.getId(), left.getValue(), right.getValue(),                NullValue.getInstance()));        }        rightSet.remove(left);      }    }  }  private static class ClusterSizeFilterFunction extends RichFilterFunction<Vertex<Long, ObjectMap>> {    private final int size;    private Integer maxClusterSize = null;    private Collection<Integer> counts;    @Override    public void open(Configuration parameters) throws Exception {      this.counts = getRuntimeContext().getBroadcastVariable("counter");    }    /**     * Get all clusters up to size minus maximum cluster size.     * @param size     * @param maxClusterSize     */    public ClusterSizeFilterFunction(int size, int maxClusterSize) {      this.size = size;      this.maxClusterSize = maxClusterSize;    }    /**     * Get all clusters with exactly this size.     * @param size     */    public ClusterSizeFilterFunction(int size) {      this.size = size;    }    @Override    public boolean filter(Vertex<Long, ObjectMap> vertex) throws Exception {      if (maxClusterSize != null) {        return vertex.getValue().getVerticesList().size() <= maxClusterSize - size;      } else {        return vertex.getValue().getVerticesList().size() == size;      }    }  }  private static class NoEqualOntologyFilterFunction implements FilterFunction<Triplet<Long, ObjectMap, NullValue>> {    @Override    public boolean filter(Triplet<Long, ObjectMap, NullValue> triplet) throws Exception {      Set<String> srcOnts = (Set<String>) triplet.getSrcVertex().getValue().get(Utils.ONTOLOGIES);      Set<String> trgOnts = (Set<String>) triplet.getTrgVertex().getValue().get(Utils.ONTOLOGIES);      for (String srcValue : srcOnts) {        if (trgOnts.contains(srcValue)) {          return false;        }      }      return true;    }  }  private static class RefineIdExcludeFilterFunction implements FilterFunction<Triplet<Long, ObjectMap, ObjectMap>> {    @Override    public boolean filter(Triplet<Long, ObjectMap, ObjectMap> triplet) throws Exception {      return !triplet.getSrcVertex().getValue().containsKey(Utils.REFINE_ID)          && !triplet.getTrgVertex().getValue().containsKey(Utils.REFINE_ID);    }  }  private static class RefineIdFilterFunction implements FilterFunction<Triplet<Long, ObjectMap, ObjectMap>> {    @Override    public boolean filter(Triplet<Long, ObjectMap, ObjectMap> triplet) throws Exception {      return triplet.getSrcVertex().getValue().containsKey(Utils.REFINE_ID)          || triplet.getTrgVertex().getValue().containsKey(Utils.REFINE_ID);    }  }  private static class RefineIdKeySelector implements KeySelector<Vertex<Long, ObjectMap>, Long> {    @Override    public Long getKey(Vertex<Long, ObjectMap> vertex) throws Exception {      return (long) vertex.getValue().get(Utils.REFINE_ID);    }  }}