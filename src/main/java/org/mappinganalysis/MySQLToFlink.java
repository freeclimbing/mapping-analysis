package org.mappinganalysis;import com.google.common.collect.Sets;import org.apache.flink.api.common.functions.CrossFunction;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.FlatJoinFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.LocalEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.configuration.Configuration;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Triplet;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.mappinganalysis.graph.ClusterComputation;import org.mappinganalysis.graph.FlinkConnectedComponents;import org.mappinganalysis.io.JDBCDataLoader;import org.mappinganalysis.model.FlinkVertex;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.*;import org.mappinganalysis.utils.GeoDistance;import org.mappinganalysis.utils.Utils;import org.simmetrics.StringMetric;import java.util.Map;import java.util.Set;/** * Read data from MySQL database via JDBC into Apache Flink. */public class MySQLToFlink {//  private static final Logger LOG = Logger.getLogger(MySQLToFlink.class);  public static void main(String[] args) throws Exception {    Configuration conf = new Configuration();    conf.setLong("taskmanager.network.numberOfBuffers", 16384L);    ExecutionEnvironment environment = new LocalEnvironment(conf);    Graph<Long, FlinkVertex, NullValue> graph = getInputGraph(Utils.LL_FULL_NAME, environment);    /**     * 1. PREPROCESSING     * - comment line(s) if not needed     */    graph = Preprocessing.applyLinkFilterStrategy(graph);    graph = Preprocessing.applyTypePreprocessing(graph);    final DataSet<Vertex<Long, FlinkVertex>> baseVertices = graph.getVertices();    /**     * 2. INITIAL MATCHING     * - apply similarity functions, similarities are added as edge value and merged     * (if more than one similarity)     */    final DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> accumulatedSimValues        = initialSimilarityComputation(graph.getTriplets());    // update triplets with edge props//    DataSet<Edge<Long, Map<String, Object>>> joinedEdges = graph//        .getEdges()//        .join(accumulatedSimValues)//        .where(0, 1).equalTo(0, 1)//        .with(new CcResultEdgesJoin());    /**     * 3. INITIAL CLUSTERING     * - connected components     */    final DataSet<Tuple2<Long, Long>> ccEdges = graph.getEdges().project(0, 1);    final DataSet<Long> ccVertices = baseVertices.map(new CcVerticesCreator());    FlinkConnectedComponents connectedComponents = new FlinkConnectedComponents(environment);    final DataSet<Tuple2<Long, Long>> ccResult = connectedComponents.compute(ccVertices, ccEdges, 1000);    // update all vertices with ccId    DataSet<Vertex<Long, FlinkVertex>> ccResultVertices = baseVertices        .join(ccResult)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    // get all/new edges in components    DataSet<Edge<Long, NullValue>> newEdges        = ClusterComputation.restrictToNewEdges(graph.getEdges(),        ClusterComputation.computeComponentEdges(ccResultVertices));    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> newSimValues        = initialSimilarityComputation(Graph.fromDataSet(        baseVertices, newEdges, environment).getTriplets());    DataSet<Tuple2<Long, Long>> newSimValuesSimple = newSimValues.project(0, 1);    DataSet<Tuple2<Long, Long>> accSimValuesSimple = accumulatedSimValues.project(0, 1);    DataSet<Tuple2<Long, Long>> newCcEdges = newSimValuesSimple.union(accSimValuesSimple);    // 2. time cc    DataSet<Tuple2<Long, Long>> newCcResult = connectedComponents        .compute(ccVertices, newCcEdges, 1000);    DataSet<Vertex<Long, FlinkVertex>> newCcResultVertices = baseVertices        .join(newCcResult)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    /**     * 4. Determination of cluster representative     * - currently: entity from best "data source" (GeoNames > DBpedia > others)     */    final DataSet<Vertex<Long, FlinkVertex>> mergedCluster = newCcResultVertices        .groupBy(new CcIdKeySelector())        .reduceGroup(new BestDataSourceAllLabelsGroupReduceFunction());//        Stats.printLabelsForMergedClusters(mergedCluster);//        Stats.countPrintResourcesPerCc(newCcResult);    // Cluster refinement//    DataSet<Vertex<Long, GeoCode>> mergedGeoCluster = mergedCluster//        .flatMap(new FlatMapFunction<Vertex<Long, FlinkVertex>, Vertex<Long, GeoCode>>() {//          @Override//          public void flatMap(Vertex<Long, FlinkVertex> vertex, Collector<Vertex<Long, GeoCode>> collector) throws Exception {//            if (vertex.getValue().hasGeoCoords()) {//              collector.collect(new Vertex<>(vertex.getId(), vertex.getValue().getGeoCoords()));//            }//          }//        });//    mergedCluster.filter(new FilterFunction<Vertex<Long, FlinkVertex>>() {//      @Override//      public boolean filter(Vertex<Long, FlinkVertex> vertex) throws Exception {//         return !(vertex.getValue().getProperties().get(Utils.CL_VERTICES) instanceof List);//      }//    });//    DataSet<Edge<Long, Double>> edgesCrossedClusters = mergedCluster        .cross(mergedCluster)        .with(new ClusterEdgeCreationCrossFunction())        .filter(new FilterFunction<Edge<Long, Double>>() {          @Override          public boolean filter(Edge<Long, Double> edge) throws Exception {            return edge.getValue() > 0.7;          }        });    edgesCrossedClusters.print();//    System.out.println(edgesCrossedClusters.count());  }  public static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  initialSimilarityComputation(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return joinDifferentSimilarityValues(basicGeoSimilarity(baseTriplets),        basicTrigramSimilarity(baseTriplets),        basicTypeSimilarity(baseTriplets));  }  @SafeVarargs  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  joinDifferentSimilarityValues(DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>... tripletDataSet) {    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> triplets = null;    boolean first = false;    for (DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> dataSet : tripletDataSet) {      if (!first) {        triplets = dataSet;        first = true;      } else {        triplets = triplets            .fullOuterJoin(dataSet)            .where(0, 1)            .equalTo(0, 1)            .with(new JoinSimilarityValueFunction());      }    }    return triplets;  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicTypeSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .map(new TypeSimilarityMapper())        .filter(new TypeFilter());  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicTrigramSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .map(new TrigramSimilarityMapper())        .filter(new TrigramSimilarityFilter());  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicGeoSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .filter(new EmptyGeoCodeFilter())        .map(new GeoCodeSimFunction())        .filter(new GeoCodeThreshold());  }  /**   * Create the input graph for further analysis,   * restrict to edges where source and target are in vertices set.   * @return graph with vertices and edges.   * @throws Exception   * @param fullDbString complete server+port+db string   */  public static Graph<Long, FlinkVertex, NullValue> getInputGraph(      String fullDbString, ExecutionEnvironment environment) throws Exception {    JDBCDataLoader loader = new JDBCDataLoader(environment);    DataSet<Vertex<Long, FlinkVertex>> vertices = loader.getVertices(fullDbString)        .map(new VertexCreator());    // restrict edges to these where source and target are vertices    DataSet<Edge<Long, NullValue>> edges = loader.getEdges(fullDbString)        .leftOuterJoin(vertices)        .where(0).equalTo(0)        .with(new EdgeRestrictFlatJoinFunction())        .leftOuterJoin(vertices)        .where(1).equalTo(0)        .with(new EdgeRestrictFlatJoinFunction());    // delete vertices without any edges due to restriction    DataSet<Vertex<Long, FlinkVertex>> left = vertices        .leftOuterJoin(edges)        .where(0).equalTo(0)        .with(new VertexRestrictFlatJoinFunction()).distinct(0);    DataSet<Vertex<Long, FlinkVertex>> finalVertices = vertices        .leftOuterJoin(edges)        .where(0).equalTo(1)        .with(new VertexRestrictFlatJoinFunction()).distinct(0)        .union(left);    return Graph.fromDataSet(finalVertices, edges, environment);  }  private static class EdgeRestrictFlatJoinFunction implements FlatJoinFunction<Edge<Long, NullValue>,      Vertex<Long, FlinkVertex>, Edge<Long, NullValue>> {    @Override    public void join(Edge<Long, NullValue> edge, Vertex<Long, FlinkVertex> vertex,                     Collector<Edge<Long, NullValue>> collector) throws Exception {      if (vertex != null) {        collector.collect(edge);      }    }  }  private static class VertexRestrictFlatJoinFunction implements FlatJoinFunction<Vertex<Long, FlinkVertex>,      Edge<Long, NullValue>, Vertex<Long, FlinkVertex>> {    @Override    public void join(Vertex<Long, FlinkVertex> vertex, Edge<Long, NullValue> edge,                     Collector<Vertex<Long, FlinkVertex>> collector) throws Exception {      if (edge != null) {        collector.collect(vertex);      }    }  }  private static class ClusterEdgeCreationCrossFunction implements CrossFunction<Vertex<Long, FlinkVertex>,      Vertex<Long, FlinkVertex>, Edge<Long, Double>> {    @Override    public Edge<Long, Double> cross(Vertex<Long, FlinkVertex> left,                                           Vertex<Long, FlinkVertex> right) throws Exception {      double similarity;      if ((long) left.getId() == right.getId()          || left.getId() > right.getId()) {        similarity = 0;      } else {        similarity = labelSim(left, right) * 0.4            + geoSim(left, right, 20) * 0.4            + typeSim(left, right) * 0.2;      }      return new Edge<>(left.getId(), right.getId(), similarity);    }    private Double typeSim(Vertex<Long, FlinkVertex> left, Vertex<Long, FlinkVertex> right) {      if (left.getValue().hasType() && right.getValue().hasType()) {        if (left.getValue().getProperties().get(Utils.TYPE)          .equals(right.getValue().getProperties().get(Utils.TYPE))) {          return 1.0;        }      }      return 0.0;    }    /**     * Compute geo coordinate similarity, norm it to a distance given in advance.     * @param left left vertex     * @param right right vertex     * @param maxDist max distance (in km) which is used for normalization     * @return normalized distance value     */    private Double geoSim(Vertex<Long, FlinkVertex> left, Vertex<Long, FlinkVertex> right, int maxDist) {      if (left.getValue().hasGeoCoords() && right.getValue().hasGeoCoords()) {        Map<String, Object> source = left.getValue().getProperties();        Map<String, Object> target = right.getValue().getProperties();        Double distance = GeoDistance.distance(Utils.getDouble(source.get(Utils.LAT)),            Utils.getDouble(source.get(Utils.LON)),            Utils.getDouble(target.get(Utils.LAT)),            Utils.getDouble(target.get(Utils.LON)));        return distance > maxDist*1000 ? 0 : (maxDist - distance) / maxDist;      }      return 0.0;    }    /**     * Get highest label similarity of two vertices, both of them have either single label or set of labels.     * @param left left vertex     * @param right right vertex     * @return trigram similarity value     */    private Double labelSim(Vertex<Long, FlinkVertex> left, Vertex<Long, FlinkVertex> right) {      StringMetric metric = Utils.getTrigramMetric(false);      double sim = 0;      if (left.getValue().hasLabel() && right.getValue().hasLabel()) {        Object leftLabel = left.getValue().getLabel();        Set<String> leftValues = null;        Object rightLabel = right.getValue().getLabel();        Set<String> rightValues = null;        if (leftLabel instanceof Set) {          leftValues = Sets.newHashSet((Set<String>) leftLabel);        }        if (rightLabel instanceof Set) {          rightValues = Sets.newHashSet((Set<String>) rightLabel);        }        if (leftValues != null) {          for (String lValue : leftValues) {            if (lValue != null) {              sim = Math.max(sim, getSimForSingleLeftValue(metric, rightLabel, rightValues, lValue));            }          }        } else {          sim = Math.max(sim, getSimForSingleLeftValue(metric, rightLabel, rightValues, (String) leftLabel));        }      }      return sim < 0.5 ? 0 : sim;    }    private Double getSimForSingleLeftValue(StringMetric metric, Object rightLabel, Set<String> rightValues, String lValue) {      if (rightValues != null) {        double tmpSim = 0;        for (String rValue : rightValues) {          if (rValue != null) {            tmpSim = Math.max(tmpSim, metric.compare(rValue, lValue));          }        }        return tmpSim;      } else {        return rightLabel != null ? (double) metric.compare((String) rightLabel, lValue) : 0;      }    }  }}