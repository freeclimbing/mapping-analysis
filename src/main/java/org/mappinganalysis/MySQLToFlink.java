package org.mappinganalysis;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.ProjectOperator;import org.apache.flink.api.java.operators.UnionOperator;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Triplet;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.mappinganalysis.graph.FlinkConnectedComponents;import org.mappinganalysis.graph.TransitiveClosureNaive;import org.mappinganalysis.io.JDBCDataLoader;import org.mappinganalysis.model.FlinkVertex;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.*;import org.mappinganalysis.utils.Stats;import org.mappinganalysis.utils.Utils;import java.util.Map;/** * Read data from MySQL database via JDBC into Apache Flink. */public class MySQLToFlink {//  private static final Logger LOG = Logger.getLogger(MySQLToFlink.class);  public static void main(String[] args) throws Exception {    ExecutionEnvironment environment = ExecutionEnvironment.createLocalEnvironment();    // check if each edge points to existing vertices    // System.out.println(graph.validate(new InvalidVertexIdsValidator<Integer, String, NullValue>()));    Graph<Long, FlinkVertex, NullValue> graph = getInputGraph(Utils.GEO_FULL_NAME);    /**     * 1. PREPROCESSING     * - comment line(s) if not needed     */    graph = Preprocessing.applyLinkFilterStrategy(graph);    graph = Preprocessing.applyTypePreprocessing(graph);    /**     * 2. INITIAL MATCHING     * - apply similarity functions, similarities are added as edge value and merged (if more than one similarity)     */    final DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets = graph.getTriplets();    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> accumulatedSimValues        = initialSimilarityComputation(baseTriplets);    // update triplets with edge props//    DataSet<Edge<Long, Map<String, Object>>> joinedEdges = graph//        .getEdges()//        .join(accumulatedSimValues)//        .where(0, 1).equalTo(0, 1)//        .with(new CcResultEdgesJoin());    final DataSet<Tuple2<Long, Long>> ed = graph.getEdges().project(0, 1);    // check all pairs in cluster    //    DataSet<Tuple2<Long, Long>> propEdges = graph.getEdges().project(0, 1);    DataSet<Tuple2<Long, Long>> newSimpleEdges        = TransitiveClosureNaive.restrictToNewEdges(ed, TransitiveClosureNaive.compute(ed, 10));    DataSet<Edge<Long, NullValue>> newEdges = newSimpleEdges        .map(new MapFunction<Tuple2<Long, Long>, Edge<Long, NullValue>>() {          @Override          public Edge<Long, NullValue> map(Tuple2<Long, Long> tuple2) throws Exception {            return new Edge<>(tuple2.f0, tuple2.f1, NullValue.getInstance());          }        });    Graph<Long, FlinkVertex, NullValue> newEdgesGraph        = Graph.fromDataSet(graph.getVertices(), newEdges, environment);    final DataSet<Triplet<Long, FlinkVertex, NullValue>> newEdgeTriplets = newEdgesGraph.getTriplets();    final DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> missingClusterEdgesSim        = initialSimilarityComputation(newEdgeTriplets);//    final DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> missingClusterEdgesSim//        = simCompNotExistingEdges(environment, graph, ed);    missingClusterEdgesSim.print();//    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> unionTriplets//        = accumulatedSimValues.union(missingClusterEdgesSim);//    System.out.println(accumulatedSimValues.count()); //4985//    System.out.println(missingClusterEdgesSim.count()); //4277//    System.out.println(unionTriplets.count()); // 9262//    System.out.println(unionTriplets.project(0, 1).distinct().count());    /**     * 3. INITIAL CLUSTERING     * - connected components     */    final DataSet<Tuple2<Long, Long>> ccEdges = accumulatedSimValues.project(0, 1);//    ccEdges.print();    final DataSet<Long> ccVertices = graph.getVertices().map(new CcVerticesCreator());//    ccVertices.print();    FlinkConnectedComponents connectedComponents = new FlinkConnectedComponents(environment);    final DataSet<Tuple2<Long, Long>> ccResult = connectedComponents        .compute(ccVertices, ccEdges, 1000);    ccResult.print();    final DataSet<Tuple2<Long, Long>> missEdges = missingClusterEdgesSim.project(0, 1);//    ccEdges.print();    final DataSet<Long> missVerts = graph.getVertices().map(new CcVerticesCreator());//    ccVertices.print();    FlinkConnectedComponents missConnectedComponents = new FlinkConnectedComponents(environment);    final DataSet<Tuple2<Long, Long>> missCcResult = missConnectedComponents        .compute(missVerts, missEdges, 1000);    missCcResult.print();    // update all vertices with ccId    DataSet<Vertex<Long, FlinkVertex>> vertices = graph.getVertices()        .join(ccResult)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    /**     * 4. Determination of cluster representative     * - currently: entity from best "data source" (GeoNames > DBpedia > others)     */    DataSet<Vertex<Long, FlinkVertex>> mergedCluster = vertices        .groupBy(new CcIdKeySelector())        .reduceGroup(new BestDataSourceGroupReduceFunction());    // Cluster refinement    // try merge clusters based on cluster representative / split, if too much diff//    Stats.printLabelsForMergedClusters(mergedCluster);    Stats.countPrintResourcesPerCc(ccResult);// first try community detection... needed?//    DataSet<Edge<Long, Double>> clusterEdges = joinedEdges//        .map(new MapFunction<Edge<Long, Map<String, Object>>, Edge<Long, Double>>() {//          @Override//          public Edge<Long, Double> map(Edge<Long, Map<String, Object>> edge) throws Exception {//            // TODO -1 rly good here?//            return new Edge<>(edge.getSource(),//                edge.getTarget(),//                edge.getValue().containsKey("distance") ? (double) edge.getValue().get("distance") : -1);//          }//        });//    DataSet<Vertex<Long, Long>> clusterVertices = vertices//        .map(new MapFunction<Vertex<Long, FlinkVertex>, Vertex<Long, Long>>() {//          @Override//          public Vertex<Long, Long> map(Vertex<Long, FlinkVertex> vertex) throws Exception {//            // fix 2. id//            return new Vertex<>(vertex.getId(), vertex.getId());//          }//        });//    Graph<Long, Long, Double> resultGraph = Graph//        .fromDataSet(clusterVertices, clusterEdges, ExecutionEnvironment.createLocalEnvironment());////// run Label Propagation for 30 iterations to detect communities on the input graph////    DataSet<Vertex<Long, Long>> communityVertices =//    Graph<Long, Long, Double> commGraph = resultGraph.run(new CommunityDetection<Long>(30, 0.5));  }  public static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  simCompNotExistingEdges(ExecutionEnvironment environment,                          Graph<Long, FlinkVertex, NullValue> graph) throws Exception {    DataSet<Tuple2<Long, Long>> propEdges = graph.getEdges().project(0, 1);    DataSet<Tuple2<Long, Long>> newSimpleEdges        = TransitiveClosureNaive.restrictToNewEdges(propEdges, TransitiveClosureNaive.compute(propEdges, 10));    DataSet<Edge<Long, NullValue>> newEdges = newSimpleEdges        .map(new MapFunction<Tuple2<Long, Long>, Edge<Long, NullValue>>() {          @Override          public Edge<Long, NullValue> map(Tuple2<Long, Long> tuple2) throws Exception {            return new Edge<>(tuple2.f0, tuple2.f1, NullValue.getInstance());          }        });    Graph<Long, FlinkVertex, NullValue> newEdgesGraph        = Graph.fromDataSet(graph.getVertices(), newEdges, environment);    final DataSet<Triplet<Long, FlinkVertex, NullValue>> newEdgeTriplets = newEdgesGraph.getTriplets();    newEdgeTriplets.print();    return initialSimilarityComputation(newEdgeTriplets);  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  initialSimilarityComputation(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return joinDifferentSimilarityValues(basicGeoSimilarity(baseTriplets),        basicTrigramSimilarity(baseTriplets),        basicTypeSimilarity(baseTriplets));  }  @SafeVarargs  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  joinDifferentSimilarityValues(DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>... tripletDataSet) {    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> triplets = null;    boolean first = false;    for (DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> dataSet : tripletDataSet) {      if (!first) {        triplets = dataSet;        first = true;      } else {        triplets = triplets            .fullOuterJoin(dataSet)            .where(0, 1)            .equalTo(0, 1)            .with(new JoinSimilarityValueFunction());      }    }    return triplets;  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicTypeSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .map(new TypeSimilarityMapper())        .filter(new TypeFilter());  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicTrigramSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .map(new TrigramSimilarityMapper())        .filter(new TrigramSimilarityFilter());  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> basicGeoSimilarity(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    return baseTriplets        .filter(new EmptyGeoCodeFilter())        .map(new GeoCodeSimFunction())        .filter(new GeoCodeThreshold());  }  /**   * Create the input graph for further analysis.   * @return graph with vertices and edges.   * @throws Exception   * @param fullDbString complete server+port+db string   */  public static Graph<Long, FlinkVertex, NullValue> getInputGraph(String fullDbString) throws Exception {    ExecutionEnvironment environment = ExecutionEnvironment.createLocalEnvironment();    JDBCDataLoader loader = new JDBCDataLoader(environment);    DataSet<Edge<Long, NullValue>> edges = loader.getEdges(fullDbString);    DataSet<Vertex<Long, FlinkVertex>> vertices = loader.getVertices(fullDbString)        .map(new VertexCreator());    return Graph.fromDataSet(vertices, edges, environment);  }}