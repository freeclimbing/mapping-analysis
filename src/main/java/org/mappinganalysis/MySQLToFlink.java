package org.mappinganalysis;import com.google.common.collect.Maps;import com.google.common.primitives.Doubles;import org.apache.flink.api.common.functions.FilterFunction;import org.apache.flink.api.common.functions.GroupReduceFunction;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Triplet;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.apache.log4j.Logger;import org.mappinganalysis.graph.FlinkConnectedComponents;import org.mappinganalysis.io.JDBCDataLoader;import org.mappinganalysis.model.FlinkVertex;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.*;import org.mappinganalysis.utils.Utils;import java.util.List;import java.util.Map;/** * Read data from MySQL database via JDBC into Apache Flink. */public class MySQLToFlink {  private static final Logger LOG = Logger.getLogger(MySQLToFlink.class);  public static void main(String[] args) throws Exception {    // check if each edge points to existing vertices    // System.out.println(graph.validate(new InvalidVertexIdsValidator<Integer, String, NullValue>()));    Graph<Long, FlinkVertex, NullValue> graph = getInputGraph();    /**     * 1. PREPROCESSING     * - comment line if not needed     */    graph = Preprocessing.applyLinkFilterStrategy(graph);    graph = Preprocessing.applyTypePreprocessing(graph);    /**     * 2. INITIAL MATCHING     * - apply similarity functions, similarities are added as edge value and merged (if more than one similarity)     */    final DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets = graph.getTriplets();    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> accumulatedSimValues        = initialSimilarityComputation(baseTriplets);    /**     * 3. INITIAL CLUSTERING     * - connected components     */    final DataSet<Tuple2<Long, Long>> ccEdges = accumulatedSimValues.project(0, 1);    final DataSet<Long> ccVertices = graph.getVertices().map(new CcVerticesCreator());    FlinkConnectedComponents connectedComponents = new FlinkConnectedComponents();    final DataSet<Tuple2<Long, Long>> ccResult = connectedComponents        .compute(ccVertices, ccEdges, 1000);    // update all vertices with ccId    DataSet<Vertex<Long, FlinkVertex>> vertices = graph.getVertices()        .join(ccResult)        .where(0).equalTo(0)        .with(new CcResultVerticesJoin());    // update triplets with edge props    DataSet<Edge<Long, Map<String, Object>>> joinedEdges = graph        .getEdges()        .join(accumulatedSimValues)        .where(0, 1).equalTo(0, 1)        .with(new CcResultEdgesJoin());    DataSet<Vertex<Long, FlinkVertex>> mergedCluster = vertices        .groupBy(new KeySelector<Vertex<Long, FlinkVertex>, Long>() {          @Override          public Long getKey(Vertex<Long, FlinkVertex> vertex) throws Exception {            return (long) vertex.getValue().getProperties().get(Utils.CC_ID);          }        })        .reduceGroup(new BestDataSourceGroupReduceFunction());    mergedCluster.print();    System.out.println(mergedCluster.count());// first try community detection... needed?//    DataSet<Edge<Long, Double>> clusterEdges = joinedEdges//        .map(new MapFunction<Edge<Long, Map<String, Object>>, Edge<Long, Double>>() {//          @Override//          public Edge<Long, Double> map(Edge<Long, Map<String, Object>> edge) throws Exception {//            // TODO -1 rly good here?//            return new Edge<>(edge.getSource(),//                edge.getTarget(),//                edge.getValue().containsKey("distance") ? (double) edge.getValue().get("distance") : -1);//          }//        });//    DataSet<Vertex<Long, Long>> clusterVertices = vertices//        .map(new MapFunction<Vertex<Long, FlinkVertex>, Vertex<Long, Long>>() {//          @Override//          public Vertex<Long, Long> map(Vertex<Long, FlinkVertex> vertex) throws Exception {//            // fix 2. id//            return new Vertex<>(vertex.getId(), vertex.getId());//          }//        });//    Graph<Long, Long, Double> resultGraph = Graph//        .fromDataSet(clusterVertices, clusterEdges, ExecutionEnvironment.createLocalEnvironment());////// run Label Propagation for 30 iterations to detect communities on the input graph////    DataSet<Vertex<Long, Long>> communityVertices =//    Graph<Long, Long, Double> commGraph = resultGraph.run(new CommunityDetection<Long>(30, 0.5));        countPrintResourcesPerCc(ccResult);//        countPrintGeoPointsPerOntology();  }  private static DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>>  initialSimilarityComputation(DataSet<Triplet<Long, FlinkVertex, NullValue>> baseTriplets) {    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> geoSimilarity        = baseTriplets        .filter(new EmptyGeoCodeFilter())        .map(new GeoCodeSimFunction())        .filter(new GeoCodeThreshold());    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> exactSim        = baseTriplets        .map(new TrigramSimilarityMapper())        .filter(new TripletFilter());    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> typeSim        = baseTriplets        .map(new TypeSimilarityMapper())        .filter(new TypeFilter());    DataSet<Triplet<Long, FlinkVertex, Map<String, Object>>> accumulatedSimValues = geoSimilarity        .fullOuterJoin(exactSim)        .where(0, 1)        .equalTo(0, 1)        .with(new JoinSimilarityValueFunction());    accumulatedSimValues = accumulatedSimValues        .fullOuterJoin(typeSim)        .where(0, 1)        .equalTo(0, 1)        .with(new JoinSimilarityValueFunction());    return accumulatedSimValues;  }  // duplicate methods in emptygeocodefilter  private static void countPrintGeoPointsPerOntology() throws Exception {    Graph<Long, FlinkVertex, NullValue> tgraph = getInputGraph();    tgraph.getVertices()        .filter(new FilterFunction<Vertex<Long, FlinkVertex>>() {          @Override          public boolean filter(Vertex<Long, FlinkVertex> vertex) throws Exception {            Map<String, Object> props = vertex.getValue().getProperties();            if (props.containsKey("lat") && props.containsKey("lon")) {              Object lat = props.get("lat");              Object lon = props.get("lon");              return ((getDouble(lat) == null) || (getDouble(lon) == null)) ? Boolean.FALSE : Boolean.TRUE;            } else {              return Boolean.FALSE;            }          }          private Double getDouble(Object latlon) {            if (latlon instanceof List) {              return Doubles.tryParse(((List) latlon).get(0).toString());            } else {              return Doubles.tryParse(latlon.toString());            }          }        })        .groupBy(new KeySelector<Vertex<Long, FlinkVertex>, String>() {          @Override          public String getKey(Vertex<Long, FlinkVertex> vertex) throws Exception {            return (String) vertex.getValue().getProperties().get(Utils.ONTOLOGY);          }        })        .reduceGroup(new GroupReduceFunction<Vertex<Long, FlinkVertex>, Vertex<Long, FlinkVertex>>() {          @Override          public void reduce(Iterable<Vertex<Long, FlinkVertex>> iterable, Collector<Vertex<Long, FlinkVertex>> collector) throws Exception {            long count = 0;            FlinkVertex result = new FlinkVertex();            Map<String, Object> resultProps = Maps.newHashMap();            boolean isVertexPrepared = false;            for (Vertex<Long, FlinkVertex> vertex : iterable) {              count++;              if (!isVertexPrepared) {                resultProps = vertex.getValue().getProperties();                result.setId(vertex.getId());                isVertexPrepared = true;              }            }            resultProps.put("count", count);            result.setProperties(resultProps);            collector.collect(new Vertex<>(result.getId(), result));          }        })        .print();  }  /**   * Count resources per component for a given flink connected component result set.   * @param ccResult dataset to be analyzed   * @throws Exception   */  private static void countPrintResourcesPerCc(DataSet<Tuple2<Long, Long>> ccResult) throws Exception {    System.out.println(ccResult.project(1).distinct().count());    List<Tuple2<Long, Long>> ccGeoList = ccResult        .groupBy(1)        .reduceGroup(new GroupReduceFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {          @Override          public void reduce(Iterable<Tuple2<Long, Long>> component, Collector<Tuple2<Long, Long>> out) throws Exception {            long count = 0;            long id = 0;            for (Tuple2<Long, Long> vertex : component) {              if (vertex.f1 == 4794 || vertex.f1 == 5680) {                System.out.println(vertex);              }              count++;              id = vertex.f1;            }            out.collect(new Tuple2<>(id, count));          }        })        .collect();    int one = 0;    int two = 0;    int three = 0;    int four = 0;    int five = 0;    int six = 0;    int seven = 0;    for (Tuple2<Long, Long> tuple2 : ccGeoList) {      if (tuple2.f1 == 1) {        one++;      } else if (tuple2.f1 == 2) {        two++;      } else if (tuple2.f1 == 3) {        three++;      } else if (tuple2.f1 == 4) {        four++;      } else if (tuple2.f1 == 5) {        five++;      } else if (tuple2.f1 == 6) {        six++;      } else if (tuple2.f1 == 7) {        seven++;      }    }    System.out.println("one: " + one + " two: " + two + " three: " + three +        " four: " + four + " five: " + five + " six: " + six + " seven: " + seven);  }  /**   * Create the input graph for further analysis.   * @return graph with vertices and edges.   * @throws Exception   */  public static Graph<Long, FlinkVertex, NullValue> getInputGraph() throws Exception {    ExecutionEnvironment environment = ExecutionEnvironment.createLocalEnvironment();    JDBCDataLoader loader = new JDBCDataLoader(environment);    DataSet<Edge<Long, NullValue>> edges = loader.getEdges();    DataSet<Vertex<Long, FlinkVertex>> vertices = loader.getVertices()        .map(new VertexCreator());    return Graph.fromDataSet(vertices, edges, environment);  }  public static class CcResultVerticesJoin implements JoinFunction<Vertex<Long, FlinkVertex>,  Tuple2<Long, Long>, Vertex<Long, FlinkVertex>> {    @Override    public Vertex<Long, FlinkVertex> join(Vertex<Long, FlinkVertex> vertex, Tuple2<Long, Long> tuple)    throws Exception {      vertex.getValue().getProperties().put(Utils.CC_ID, tuple.f1);      return vertex;    }  }  public static class CcResultEdgesJoin implements JoinFunction<Edge<Long,NullValue>,      Triplet<Long,FlinkVertex,Map<String,Object>>, Edge<Long, Map<String, Object>>> {    @Override    public Edge<Long, Map<String, Object>> join(Edge<Long, NullValue> edge,        Triplet<Long, FlinkVertex, Map<String, Object>> triplet)    throws Exception {      return new Edge<>(edge.getSource(), edge.getTarget(), triplet.f4);    }  }  public static class FilterExactMatch implements FilterFunction<Triplet<Long, FlinkVertex, Map<String, Object>>> {    @Override    public boolean filter(Triplet<Long, FlinkVertex, Map<String, Object>> triplet) throws Exception {      return triplet.getEdge().getValue().containsKey("exactMatch");    }  }  public static class FilterSizeMinTwo implements FilterFunction<Triplet<Long, FlinkVertex, Map<String, Object>>> {    @Override    public boolean filter(Triplet<Long, FlinkVertex, Map<String, Object>> triplet) throws Exception {      return triplet.getEdge().getValue().size() > 1;    }  }  private static class JoinFilterStrategyFunction      implements JoinFunction<Edge<Long, NullValue>, Edge<Long, NullValue>, Edge<Long, NullValue>> {    @Override    public Edge<Long, NullValue> join(Edge<Long, NullValue> edge, Edge<Long, NullValue> deleteEdge) throws Exception {      return edge;    }  }  private static class LabelExtractor implements MapFunction<FlinkVertex, Vertex<Long, String>> {    @Override    public Vertex<Long, String> map(FlinkVertex flinkVertex) throws Exception {      Object label = flinkVertex.getValue().get("label");      return new Vertex<>(flinkVertex.getId(), (label != null) ? label.toString() : "null");    }  }  private static class GeoCodeThreshold      implements FilterFunction<Triplet<Long, FlinkVertex, Map<String, Object>>> {    @Override    public boolean filter(Triplet<Long, FlinkVertex, Map<String, Object>> distanceThreshold) throws Exception {      return ((double) distanceThreshold.getEdge().getValue().get("distance")) < 50000;    }  }  private static class CcVerticesCreator implements MapFunction<Vertex<Long, FlinkVertex>, Long> {    @Override    public Long map(Vertex<Long, FlinkVertex> flinkVertex) throws Exception {      return flinkVertex.getId();    }  }}