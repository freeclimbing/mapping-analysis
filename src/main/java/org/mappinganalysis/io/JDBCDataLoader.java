package org.mappinganalysis.io;import com.google.common.primitives.Doubles;import org.apache.flink.api.common.functions.FlatJoinFunction;import org.apache.flink.api.common.functions.GroupReduceFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.typeinfo.BasicTypeInfo;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.io.TextOutputFormat;import org.apache.flink.api.java.io.jdbc.JDBCInputFormat;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.api.java.typeutils.TupleTypeInfo;import org.apache.flink.core.fs.FileSystem;import org.apache.flink.core.fs.Path;import org.apache.flink.graph.Edge;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.apache.log4j.Logger;import org.mappinganalysis.io.functions.BasicVertexCreator;import org.mappinganalysis.io.functions.FlinkEdgeCreator;import org.mappinganalysis.io.functions.FlinkPropertyMapper;import org.mappinganalysis.io.functions.PropertyCoGroupFunction;import org.mappinganalysis.model.FlinkProperty;import org.mappinganalysis.model.ObjectMap;import org.mappinganalysis.utils.Utils;import java.util.Locale;import java.util.ResourceBundle;/** * JDBC Data Loader for Flink */public class JDBCDataLoader {  private static final Logger LOG = Logger.getLogger(JDBCDataLoader.class);  private ExecutionEnvironment env;  private final ResourceBundle prop;  public JDBCDataLoader(ExecutionEnvironment env) {    this.env = env;    this.prop = ResourceBundle.getBundle(Utils.DB_PROPERY_FILE_NAME,        Locale.getDefault(),        Thread.currentThread().getContextClassLoader());  }  public DataSet<Vertex<Long, ObjectMap>> getVerticesFromCsv(final String vertexFile, String propertyFile, String inputDir) throws Exception {    DataSet<Tuple3<Integer, String, String>> vertices = env.readCsvFile(vertexFile)        .fieldDelimiter(";")        .ignoreInvalidLines()        .types(Integer.class, String.class, String.class);    TextOutputFormat format = new TextOutputFormat(new Path(inputDir + "baseVerts"));    format.setWriteMode(FileSystem.WriteMode.OVERWRITE);    DataSet<Vertex<Long, ObjectMap>> baseVerts = vertices.map(new BasicVertexCreator());    baseVerts.map(new MapFunction<Vertex<Long,ObjectMap>, String>() {      @Override      public String map(Vertex<Long, ObjectMap> vertex) throws Exception {        return vertex.toString();      }    }).output(format);    TextOutputFormat propFormat = new TextOutputFormat(new Path(inputDir + "props"));    propFormat.setWriteMode(FileSystem.WriteMode.OVERWRITE);    DataSet<FlinkProperty> props = getPropertiesFromCsv(propertyFile);    props.map(new MapFunction<FlinkProperty, String>() {      @Override      public String map(FlinkProperty flinkProperty) throws Exception {        return flinkProperty.toString();      }    }).output(propFormat);    TextOutputFormat resultFormat = new TextOutputFormat(new Path(inputDir + "result"));    resultFormat.setWriteMode(FileSystem.WriteMode.OVERWRITE);    DataSet<Vertex<Long, ObjectMap>> result = baseVerts.coGroup(props)        .where(0)        .equalTo(0)        .with(new PropertyCoGroupFunction());    result.map(new MapFunction<Vertex<Long,ObjectMap>, String>() {      @Override      public String map(Vertex<Long, ObjectMap> vertex) throws Exception {        return vertex.toString();      }    }).output(resultFormat);    return result;//        vertices.map(new BasicVertexCreator())//        .coGroup(getPropertiesFromCsv(propertyFile))//        .where(0)//        .equalTo(0)//        .with(new PropertyCoGroupFunction());//    DataSet<Vertex<Long, ObjectMap>> map = vertices.map(new BasicVertexCreator());////    DataSet<FlinkProperty> properties = getPropertiesFromCsv(propertyFile);////    DataSet<Vertex<Long, ObjectMap>> group = properties.groupBy(0).reduceGroup(new PropertyVertexCreator());////    return group.leftOuterJoin(map)////    return getPropertiesFromCsv(propertyFile)////        .groupBy(0)////        .reduceGroup(new PropertyVertexCreator())////        .leftOuterJoin(vertices.map(new BasicVertexCreator()))//        .where(0)//        .equalTo(0)//        .with(new FlatJoinFunction<Vertex<Long,ObjectMap>, Vertex<Long,ObjectMap>, Vertex<Long, ObjectMap>>() {//          @Override//          public void join(Vertex<Long, ObjectMap> left, Vertex<Long, ObjectMap> right, Collector<Vertex<Long, ObjectMap>> collector) throws Exception {//            left.getValue().addProperty(Utils.DB_URL_FIELD, right.getValue().get(Utils.DB_URL_FIELD));//            left.getValue().addProperty(Utils.ONTOLOGY, right.getValue().get(Utils.ONTOLOGY));////            collector.collect(left);//          }//        });  }  public DataSet<FlinkProperty> getPropertiesFromCsv(String propertiesFile) throws Exception {    DataSet<Tuple4<Integer, String, String, String>> properties = env.readCsvFile(propertiesFile)        .fieldDelimiter(";")        .ignoreInvalidLines()        .types(Integer.class, String.class, String.class, String.class);    return properties.map(new FlinkPropertyMapper()).withForwardedFields("f1;f2;f3");  }  public DataSet<Edge<Long, NullValue>> getEdgesFromCsv(String edgeFile) {    return env.readCsvFile(edgeFile)        .fieldDelimiter(";")        .ignoreInvalidLines()        .types(Integer.class, Integer.class)        .map(new FlinkEdgeCreator());  }  /**   * Parses and transforms database entities to tuples.   *   * @return DataSet containing all vertices in the graph   */  @SuppressWarnings("unchecked")  public DataSet<Vertex<Long, ObjectMap>> getVertices(String fullDbName) throws Exception {    DataSet<Tuple3<Integer, String, String>> input        = env.createInput(JDBCInputFormat.buildJDBCInputFormat()            .setDrivername("com.mysql.jdbc.Driver")            .setDBUrl(prop.getString(fullDbName))            .setUsername(prop.getString("user"))            .setPassword(prop.getString("pw"))            .setQuery("select id, url, ontID_fk from concept where ontID_fk in " +                " ('http://dbpedia.org/', 'http://sws.geonames.org/', 'http://linkedgeodata.org/', " +                " 'http://data.nytimes.com/', 'http://rdf.freebase.com/')")            .finish(),        new TupleTypeInfo(Tuple3.class,            BasicTypeInfo.INT_TYPE_INFO,            BasicTypeInfo.STRING_TYPE_INFO,            BasicTypeInfo.STRING_TYPE_INFO)    );    return getProperties(fullDbName)        .groupBy(0)        .reduceGroup(new PropertyVertexCreator())        .leftOuterJoin(input.map(new BasicVertexCreator()))        .where(0)        .equalTo(0)        .with(new FlatJoinFunction<Vertex<Long,ObjectMap>, Vertex<Long,ObjectMap>, Vertex<Long, ObjectMap>>() {          @Override          public void join(Vertex<Long, ObjectMap> left, Vertex<Long, ObjectMap> right, Collector<Vertex<Long, ObjectMap>> collector) throws Exception {            left.getValue().addProperty(Utils.DB_URL_FIELD, right.getValue().get(Utils.DB_URL_FIELD));            left.getValue().addProperty(Utils.ONTOLOGY, right.getValue().get(Utils.ONTOLOGY));            collector.collect(left);          }        });  }  /**   * Parses and transforms the properties to {@link FlinkProperty} tuples.   *   * @return DataSet containing all properties from database.   */  @SuppressWarnings("unchecked")  public DataSet<FlinkProperty> getProperties(String fullDbName) {    return env.createInput(JDBCInputFormat.buildJDBCInputFormat()            .setDrivername("com.mysql.jdbc.Driver")            .setDBUrl(prop.getString(fullDbName))            .setUsername(prop.getString("user"))            .setPassword(prop.getString("pw"))            .setQuery("select id, attName, attValue, attValueType from concept_attributes")            .finish(),        new TupleTypeInfo(Tuple4.class, BasicTypeInfo.INT_TYPE_INFO,            BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO,            BasicTypeInfo.STRING_TYPE_INFO)    ).map(new FlinkPropertyMapper()).withForwardedFields("f1;f2;f3");  }  /**   * Parses and transforms the db edges to {@link Edge} tuples.   *   * @return DataSet containing all edges from the database table.   */  @SuppressWarnings("unchecked")  public DataSet<Edge<Long, NullValue>> getEdges(String fullDbName) {    DataSet<Tuple2<Integer, Integer>> input        = env.createInput(JDBCInputFormat.buildJDBCInputFormat()            .setDrivername("com.mysql.jdbc.Driver")            .setDBUrl(prop.getString(fullDbName))            .setUsername(prop.getString("user"))            .setPassword(prop.getString("pw"))            .setQuery("select srcID, trgID from linksWithIDs")            .finish(),        new TupleTypeInfo(Tuple2.class,            BasicTypeInfo.INT_TYPE_INFO,            BasicTypeInfo.INT_TYPE_INFO)    );    return input.map(new FlinkEdgeCreator());  }  private static class PropertyVertexCreator implements GroupReduceFunction<FlinkProperty, Vertex<Long, ObjectMap>> {    private final Vertex<Long, ObjectMap> reuseVertex;    public PropertyVertexCreator() {      reuseVertex = new Vertex<>();      reuseVertex.setValue(new ObjectMap());    }    @Override    public void reduce(Iterable<FlinkProperty> properties, Collector<Vertex<Long, ObjectMap>> collector) throws Exception {      boolean isIdSet = false;      for (FlinkProperty property : properties) {        if (!isIdSet) {          reuseVertex.setId(property.getVertexId());          isIdSet = true;        }        Object value = property.getPropertyValue();        String key = property.getPropertyKey();        if (reuseVertex.getValue().containsKey(Utils.LAT) || reuseVertex.getValue().containsKey(Utils.LON)) {          continue;        }        if (property.getPropertyType().equals("double")) {          value = Doubles.tryParse(value.toString());        } else if (property.getPropertyType().equals("string")) {          value = value.toString();        }        reuseVertex.getValue().addProperty(key, value);      }      if (reuseVertex.getValue().containsKey(Utils.LABEL)) {        collector.collect(reuseVertex);      }    }  }}