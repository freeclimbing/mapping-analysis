package org.mappinganalysis;import com.google.common.base.Preconditions;import com.google.common.primitives.Doubles;import org.apache.commons.cli.*;import org.apache.flink.api.common.ProgramDescription;import org.apache.flink.api.common.functions.FlatJoinFunction;import org.apache.flink.api.common.functions.GroupReduceFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.MapOperator;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.flink.util.Collector;import org.apache.log4j.Logger;import org.mappinganalysis.io.output.ExampleOutput;import org.mappinganalysis.model.ObjectMap;import org.mappinganalysis.model.Preprocessing;import org.mappinganalysis.model.functions.refinement.Refinement;import org.mappinganalysis.model.functions.representative.MajorityPropertiesGroupReduceFunction;import org.mappinganalysis.model.functions.simcomputation.SimilarityComputation;import org.mappinganalysis.util.Constants;import org.mappinganalysis.util.Utils;import org.mappinganalysis.util.functions.keyselector.HashCcIdKeySelector;/** * Mapping analysis example */public class MappingAnalysisExample implements ProgramDescription {  private static final Logger LOG = Logger.getLogger(MappingAnalysisExample.class);  private static ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  /**   * @param args cmd args   * @throws Exception   */  public static void main(String[] args) throws Exception {    Preconditions.checkArgument(        args.length == 7, "args[0]: input dir, " +            "args[1]: verbosity(less, info, debug), " +            "args[2]: isSimSortEnabled (isSimsort), " +            "args[3]: min cluster similarity (e.g., 0.7), " +            "args[4]: min SimSort similarity (e.g., 0.5)" +            "args[5]: linklion mode (all, nyt, random)" +            "args[6]: processing mode (input, preproc, analysis, eval)");    Constants.INPUT_DIR = args[0];    Constants.VERBOSITY = args[1];    Constants.IS_SIMSORT_ENABLED = args[2].equals("isSimsort");    Constants.IS_SIMSORT_ALT_ENABLED = args[2].equals("isSimsortAlt");    Constants.MIN_CLUSTER_SIM = Doubles.tryParse(args[3]);    Constants.MIN_SIMSORT_SIM = Doubles.tryParse(args[4]);    Constants.LL_MODE = args[5];    Constants.PROC_MODE = args[6];    Constants.MIN_LABEL_PRIORITY_SIM = 0.5;    /**     * Read data, execute analysis     */    ExampleOutput out = new ExampleOutput(env);    switch (Constants.PROC_MODE) {      case Constants.INPUT:        Graph<Long, ObjectMap, NullValue> graph = Preprocessing.getInputGraphFromCsv(env, out);        Utils.writeGraphToJSONFile(graph, Constants.LL_MODE.concat("InputGraph"));        env.execute();        break;      case Constants.PREPROC:        execute(Utils.readFromJSONFile(Constants.LL_MODE + "InputGraph", env), out);        break;      case Constants.ANALYSIS:        execute(Utils.readFromJSONFile(Constants.LL_MODE + "PreprocGraph", env), out);        break;      case Constants.EVAL:        executeEval(out);        break;      case Constants.STATS:        String input = Constants.LL_MODE + "InputGraph";        executeStats(input);        break;    }  }  /**   * Mapping analysis computation   */  private static <EV> void execute(Graph<Long, ObjectMap, EV> inputGraph, ExampleOutput out) throws Exception {    Constants.IGNORE_MISSING_PROPERTIES = true;    Constants.IS_LINK_FILTER_ACTIVE = true;    if (Constants.PROC_MODE.equals(Constants.PREPROC)) {      out.addVertexAndEdgeSizes("input size", inputGraph);    }    Graph<Long, ObjectMap, ObjectMap> graph = Preprocessing        .execute(inputGraph, out, env);    if (Constants.PROC_MODE.equals(Constants.PREPROC)) {//      out.addVertexAndEdgeSizes("pre clustering", inputGraph);      Utils.writeGraphToJSONFile(graph, "2-pre-clustering");    }    graph = SimilarityComputation.executeAdvanced(        graph,        Constants.DEFAULT_VALUE,        env,        out);    if (Constants.PROC_MODE.equals(Constants.ANALYSIS)) {      Utils.writeGraphToJSONFile(graph, "4-post-decomposition");      out.addVertexAndEdgeSizes("4 vertex and edge sizes post decomposition", graph);//    /* 4. Representative */      DataSet<Vertex<Long, ObjectMap>> representativeVertices = graph.getVertices()          .groupBy(new HashCcIdKeySelector())          .reduceGroup(new MajorityPropertiesGroupReduceFunction());      out.addClusterSizes("5 cluster sizes representatives", representativeVertices);      Utils.writeVerticesToJSONFile(representativeVertices, "5-representatives-json");//    /* 5. Refinement */      representativeVertices = Refinement.init(representativeVertices, out);//    out.addClusterSizes("6a cluster sizes merge init", representativeVertices);//    Utils.writeToFile(representativeVertices, "6_init_merge_vertices");      DataSet<Vertex<Long, ObjectMap>> mergedClusters = Refinement.execute(representativeVertices, out);      Utils.writeVerticesToJSONFile(mergedClusters, "6-merged-clusters-json");      out.addClusterSizes("6b cluster sizes merged", mergedClusters);      out.print();    }  }  /**   * if analysis rerun, add vertices to 5 + 6 directories   * @param out   * @throws Exception   */  private static void executeEval(ExampleOutput out) throws Exception {    DataSet<Vertex<Long, ObjectMap>> mergedClusters        = Utils.readVerticesFromJSONFile("6-merged-clusters-json", env, false);    DataSet<Vertex<Long, ObjectMap>> representativeVertices        = Utils.readVerticesFromJSONFile("5-representatives-json", env, false);    Graph<Long, ObjectMap, ObjectMap> graph        = Utils.readFromJSONFile(Constants.LL_MODE + "PreprocGraph", env);//    Stats.printResultEdgeCounts(graph.mapEdges(new MapFunction<Edge<Long, ObjectMap>, NullValue>() {//      @Override//      public NullValue map(Edge<Long, ObjectMap> value) throws Exception {//        return NullValue.getInstance();//      }//    }), out, mergedClusters);//    out.addSelectedBaseClusters("selected base clusters final values",//        graph.getVertices(),//        representativeVertices, Utils.getVertexList(dataset));    out.printEvalThreePercent("eval", mergedClusters, graph.getVertices());    out.print();//    Stats.addChangedWhileMergingVertices(out, representativeVertices, mergedClusters);//    Stats.printAccumulatorValues(env, graph); // not working if workflow is split  }  /**   * stats for workflow   */  public static void executeStats(String graphPath) throws Exception {    Graph<Long, ObjectMap, ObjectMap> graph        = Utils.readFromJSONFile(graphPath, env);    printEdgeSizes(graph);//    DataSet<Vertex<Long, ObjectMap>> mergedClusters//        = Utils.readVerticesFromJSONFile("6-merged-clusters-json", env, false);//    DataSet<Vertex<Long, ObjectMap>> representativeVertices//        = Utils.readVerticesFromJSONFile("5-representatives-json", env, false);//    Graph<Long, ObjectMap, ObjectMap> graph//        = Utils.readFromJSONFile(Constants.LL_MODE + "PreprocGraph", env);////    out.printEvalThreePercent("eval", mergedClusters, graph.getVertices());//    out.print();  }  /**   *   * remove ids earlier (ahem: at all)   * @throws Exception   */  public static void printEdgeSizes(Graph<Long, ObjectMap, ObjectMap> graph) throws Exception {    graph.getVertices().map(new MapFunction<Vertex<Long,ObjectMap>, Tuple3<Long, String, Integer>>() {      @Override      public Tuple3<Long, String, Integer> map(Vertex<Long, ObjectMap> vertex) throws Exception {        return new Tuple3<Long, String, Integer>(vertex.getId(), vertex.getValue().getOntology(), 1);      }    })    .groupBy(1)        .reduce((left, right) -> new Tuple3<>(left.f0, left.f1, left.f2 + right.f2))        .map(value -> new Tuple2<>(value.f1, value.f2))        .returns(new TypeHint<Tuple2<String, Integer>>() {})        .print();    // TODO check, error somewhere! duplicate edges?//    DataSet<Tuple2<Long, Long>> edges = graph.getEdgeIds();//    DataSet<Tuple4<Long, Long, String, String>> edgeSizes = getEdgeSizes(graph, edges);//    DataSet<Tuple5<Long, Long, String, String, Integer>> tmp = edgeSizes.map(new MapFunction<Tuple4<Long, Long, String, String>, Tuple4<Long, Long, String, String>>() {//      @Override//      public Tuple4<Long, Long, String, String> map(Tuple4<Long, Long, String, String> value) throws Exception {//        if (value.f2.equals(Constants.DBP_NS)) {//          return value;//        } else if (value.f3.equals(Constants.DBP_NS)) {//          return new Tuple4<>(value.f1, value.f0, value.f3, value.f2);//        } else if (value.f2.equals(Constants.GN_NS)) {//          return value;//        } else if (value.f3.equals(Constants.GN_NS)) {//          return new Tuple4<>(value.f1, value.f0, value.f3, value.f2);//        } else if (value.f2.equals(Constants.LGD_NS)) {//          return value;//        } else if (value.f3.equals(Constants.LGD_NS)) {//          return new Tuple4<>(value.f1, value.f0, value.f3, value.f2);//        } else if (value.f2.equals(Constants.FB_NS)) {//          return value;//        } else if (value.f3.equals(Constants.FB_NS)) {//          return new Tuple4<>(value.f1, value.f0, value.f3, value.f2);//        } else {//          LOG.info("### schould not happen");//          return value;//        }//      }//    })//        .map(value -> new Tuple5<>(value.f0, value.f1, value.f2, value.f3, 1))//        .returns(new TypeHint<Tuple5<Long, Long, String, String, Integer>>() {//        });////    tmp.groupBy(2, 3)//        .reduce((left, right) -> new Tuple5<>(left.f0, left.f1, left.f2, left.f3, left.f4 + right.f4))//        .map(value -> new Tuple3<>(value.f2, value.f3, value.f4))//        .returns(new TypeHint<Tuple3<String, String, Integer>>() {//        })//        .print();  }  private static DataSet<Tuple4<Long, Long, String, String>> getEdgeSizes(      Graph<Long, ObjectMap, ObjectMap> graph,      DataSet<Tuple2<Long, Long>> edges) {    return edges.leftOuterJoin(graph.getVertices())        .where(0)        .equalTo(0)        .with(new FlatJoinFunction<Tuple2<Long,Long>,            Vertex<Long,ObjectMap>,            Tuple3<Long, Long, String>>() {          @Override          public void join(Tuple2<Long, Long> left,                           Vertex<Long, ObjectMap> right,                           Collector<Tuple3<Long, Long, String>> out) throws Exception {            if (left != null) {              out.collect(new Tuple3<>(left.f0, left.f1, right.getValue().getOntology()));            }          }        })        .leftOuterJoin(graph.getVertices())        .where(1)        .equalTo(0)        .with(new FlatJoinFunction<Tuple3<Long,Long,String>, Vertex<Long,ObjectMap>, Tuple4<Long, Long, String, String>>() {          @Override          public void join(Tuple3<Long, Long, String> left,                           Vertex<Long, ObjectMap> right,                           Collector<Tuple4<Long, Long, String, String>> out) throws Exception {            if (left != null) {              if (left.f0 < left.f1) {                out.collect(new Tuple4<>(left.f0, left.f1, left.f2, right.getValue().getOntology()));              } else {                out.collect(new Tuple4<>(left.f1, left.f0, right.getValue().getOntology(), left.f2));              }            }          }        })        .distinct(0,1);  }  /**   * Parses the program arguments or returns help if args are empty.   *   * @param args program arguments   * @return command line which can be used in the program   */  private static CommandLine parseArguments(String[] args) throws ParseException {    CommandLineParser parser = new BasicParser();    return parser.parse(OPTIONS, args);  }// not currenctly used//CommandLine cmd = parseArguments(args);//  Constants.IS_LINK_FILTER_ACTIVE = cmd.hasOption(OPTION_LINK_FILTER_PREPROCESSING);//  Constants.IGNORE_MISSING_PROPERTIES = cmd.hasOption(OPTION_IGNORE_MISSING_PROPERTIES);//  Constants.PRE_CLUSTER_STRATEGY = cmd.getOptionValue(OPTION_PRE_CLUSTER_FILTER, Constants.DEFAULT_VALUE);  /**   * Command line options   */  private static final String OPTION_LINK_FILTER_PREPROCESSING = "lfp";  private static final String OPTION_PRE_CLUSTER_FILTER = "pcf";  private static final String OPTION_ONLY_INITIAL_CLUSTER = "oic";  private static final String OPTION_DATA_SET_NAME = "ds";  private static final String OPTION_WRITE_STATS = "ws";  private static final String OPTION_CLUSTER_STATS = "cs";  private static final String OPTION_IGNORE_MISSING_PROPERTIES = "imp";  private static final String OPTION_PROCESSING_MODE = "pm";  private static final String OPTION_TYPE_MISS_MATCH_CORRECTION = "tmmc";  private static Options OPTIONS;  static {    OPTIONS = new Options();    // general    OPTIONS.addOption(OPTION_DATA_SET_NAME, "dataset-name", true,        "Choose one of the datasets [" + Constants.CMD_GEO + " (default), " + Constants.CMD_LL + "].");    OPTIONS.addOption(OPTION_PROCESSING_MODE, "processing-mode", true,        "Choose the processing mode [SimSort + TypeGroupBy (default), simSortOnly].");    OPTIONS.addOption(OPTION_IGNORE_MISSING_PROPERTIES, "ignore-missing-properties", false,        "Do not penalize missing properties on resources in similarity computation process (default: false).");    OPTIONS.addOption(OPTION_ONLY_INITIAL_CLUSTER, "only-initial-cluster", false,        "Don't compute final clusters, stop after preprocessing (default: false).");    // Preprocessing    OPTIONS.addOption(OPTION_LINK_FILTER_PREPROCESSING, "link-filter-preprocessing", false,        "Exclude edges where vertex has several target vertices having equal dataset ontology (default: false).");    OPTIONS.addOption(OPTION_TYPE_MISS_MATCH_CORRECTION, "type-miss-match-correction", false,        "Exclude edges where directly connected source and target vertices have different type property values. " +            "(default: false).");    // todo to be changed    OPTIONS.addOption(OPTION_PRE_CLUSTER_FILTER, "pre-cluster-filter", true,        "Specify preprocessing filter strategy for entity properties ["            + Constants.DEFAULT_VALUE + " (combined), geo, label, type]");    OPTIONS.addOption(OPTION_WRITE_STATS, "write-stats", false,        "Write statistics to output (default: false).");    Option clusterStats = new Option(OPTION_CLUSTER_STATS, "cluster-stats", true,        "Be more verbose while processing specified cluster ids.");    clusterStats.setArgs(Option.UNLIMITED_VALUES);    OPTIONS.addOption(clusterStats);  }  @Override  public String getDescription() {    return MappingAnalysisExample.class.getName();  }}