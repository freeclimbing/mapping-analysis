package org.mappinganalysis;import com.google.common.base.Preconditions;import com.google.common.primitives.Doubles;import org.apache.flink.api.common.JobExecutionResult;import org.apache.flink.api.common.ProgramDescription;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.graph.Graph;import org.apache.flink.graph.Vertex;import org.apache.flink.types.NullValue;import org.apache.log4j.Logger;import org.mappinganalysis.io.impl.DataDomain;import org.mappinganalysis.io.impl.csv.CSVDataSource;import org.mappinganalysis.io.impl.json.JSONDataSink;import org.mappinganalysis.io.impl.json.JSONDataSource;import org.mappinganalysis.model.ObjectMap;import org.mappinganalysis.model.functions.decomposition.Clustering;import org.mappinganalysis.model.functions.decomposition.representative.RepresentativeCreatorMultiMerge;import org.mappinganalysis.model.functions.decomposition.simsort.SimSort;import org.mappinganalysis.model.functions.decomposition.typegroupby.TypeGroupBy;import org.mappinganalysis.model.functions.merge.MergeExecution;import org.mappinganalysis.model.functions.merge.MergeInitialization;import org.mappinganalysis.model.functions.preprocessing.DefaultPreprocessing;import org.mappinganalysis.util.Constants;import org.mappinganalysis.util.Stats;import org.mappinganalysis.util.config.IncrementalConfig;import java.util.concurrent.TimeUnit;/** * Mapping analysis example - old main class, now separated into several * programs in package analysis-examples */@Deprecatedpublic class MappingAnalysisExample implements ProgramDescription {  private static final Logger LOG = Logger.getLogger(MappingAnalysisExample.class);  private static ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  private static Double minSimilarity;  private static String inputPath;  /**   * @param args cmd args   */  public static void main(String[] args) throws Exception {    Preconditions.checkArgument(        args.length == 4, "args[0]: input dir, " +            "args[1]: min SimSort similarity (e.g., 0.7), " +            "args[2]: linklion mode (all, nyt, random)" +            "args[3]: processing mode (input, preproc, analysis, eval)");    inputPath = args[0];    Constants.SOURCE_COUNT = args[0].contains("linklion") ? 5 : 4;    minSimilarity = Doubles.tryParse(args[1]);    Constants.LL_MODE = args[2];    Constants.PROC_MODE = args[3];    switch (Constants.PROC_MODE) {      case Constants.READ_INPUT:        CSVDataSource.createInputGraphFromCsv(env);        break;      case Constants.PREPROC:        executePreprocessing();        break;      case Constants.PREPROCESSING_COMPLETE: // combination of preproc and lf        executeCompletePreprocessing();        break;      case Constants.DECOMPOSITION_COMPLETE: // combination of ic, decomp and simsort        executeCompleteDecomposition();        break;      case Constants.ANALYSIS:        executeReprCreationAndMerge();        break;      case Constants.ALL:        executePreprocessing();        executeCompleteDecomposition();        executeReprCreationAndMerge();        break;      case Constants.STATS_EDGE_INPUT:        executeStats(Constants.LL_MODE.concat(Constants.INPUT_GRAPH), "edge");        break;      case Constants.STATS_EDGE_PREPROC:        executeStats(Constants.INIT_CLUST, "edge");        break;      case Constants.STATS_VERTEX_INPUT:        executeStats(Constants.LL_MODE.concat(Constants.INPUT_GRAPH), "vertex");        break;      case Constants.STATS_VERTEX_PREPROC:        executeStats(Constants.INIT_CLUST, "vertex");        break;      case Constants.MISS:        executeStats(Constants.LL_MODE.concat(Constants.INPUT_GRAPH), "-");        break;      default:        throw new IllegalArgumentException("No implementation for " + Constants.PROC_MODE);    }  }  /**   * execute preprocessing   */  private static void executePreprocessing() throws Exception {    Graph<Long, ObjectMap, NullValue> inGraph =        new JSONDataSource(inputPath,            Constants.LL_MODE.concat(Constants.INPUT_GRAPH), env)            .getGraph(ObjectMap.class, NullValue.class);    IncrementalConfig config = new IncrementalConfig(DataDomain.GEOGRAPHY, env);    config.setMetric(Constants.COSINE_TRIGRAM);    Graph<Long, ObjectMap, ObjectMap> graph        = inGraph.run(new DefaultPreprocessing(config));    new JSONDataSink(inputPath, Constants.LL_MODE.concat(Constants.PREPROC_GRAPH))        .writeGraph(graph);    env.execute("Basic Preprocessing");  }  /**   * Preprocessing + LinkFilter together   */  private static void executeCompletePreprocessing() throws Exception {    IncrementalConfig config = new IncrementalConfig(DataDomain.GEOGRAPHY, env);    config.setMetric(Constants.COSINE_TRIGRAM);    Graph<Long, ObjectMap, ObjectMap> graph        = new JSONDataSource(inputPath,        Constants.LL_MODE.concat(Constants.INPUT_GRAPH), env)        .getGraph(ObjectMap.class, NullValue.class)        .run(new DefaultPreprocessing(config));    new JSONDataSink(inputPath, Constants.LL_MODE.concat(Constants.LF_GRAPH))        .writeGraph(graph);    JobExecutionResult result = env.execute("Preprocessing");    LOG.info("Preprocessing needed "        + result.getNetRuntime(TimeUnit.SECONDS) + " seconds.");  }  /**   * Initical Clustering, TypeGroupBy and SimSort   */  private static void executeCompleteDecomposition() throws Exception {    Graph<Long, ObjectMap, ObjectMap> graph =        new JSONDataSource(inputPath,            Constants.LL_MODE.concat(Constants.LF_GRAPH), env)            .getGraph();    graph = Clustering        .createInitialClustering(graph, env)        .run(new TypeGroupBy(env))        .run(new SimSort(DataDomain.GEOGRAPHY, Constants.COSINE_TRIGRAM, minSimilarity, env));    new JSONDataSink(inputPath, Constants.LL_MODE.concat(Constants.SIMSORT_GRAPH))        .writeGraph(graph);    JobExecutionResult result = env.execute("Decomposition");    LOG.info("Decomposition needed "        + result.getNetRuntime(TimeUnit.SECONDS) + " seconds.");  }  private static void executeReprCreationAndMerge() throws Exception {    Graph<Long, ObjectMap, ObjectMap> graph =        new JSONDataSource(inputPath,            Constants.LL_MODE.concat(Constants.SIMSORT_GRAPH), env)            .getGraph();    /* 4. Representative */    DataSet<Vertex<Long, ObjectMap>> representatives = graph.getVertices()        .runOperation(new RepresentativeCreatorMultiMerge(DataDomain.GEOGRAPHY));    /* 5. Merge */    representatives = representatives        .runOperation(new MergeInitialization(DataDomain.GEOGRAPHY))        .runOperation(new MergeExecution(DataDomain.GEOGRAPHY,            Constants.COSINE_TRIGRAM,            0.5,            Constants.SOURCE_COUNT,            env));    new JSONDataSink(inputPath, "6-merged-clusters-json")        .writeVertices(representatives);    JobExecutionResult result = env.execute("Representatives and Merge");    LOG.info("Representatives and Merge needed "        + result.getNetRuntime(TimeUnit.SECONDS) + " seconds.");  }  /**   * For vertices, get vertex count per data source.   * For edges, get edge (undirected) edge counts between sources.   */  private static void executeStats(String step, String edgeOrVertex) throws Exception {    Graph<Long, ObjectMap, NullValue> graph =        new JSONDataSource(inputPath, step, env)            .getGraph(ObjectMap.class, NullValue.class);    if (edgeOrVertex.equals("edge")) { //&& graphPath.contains("InputGraph")) {      Stats.printEdgeSourceCounts(graph)          .print();    } else if (edgeOrVertex.equals("vertex")) {      Stats.printVertexSourceCounts(graph)          .print();    } else if (Constants.PROC_MODE.equals(Constants.MISS)) {      Stats.countMissingGeoAndTypeProperties(          Constants.LL_MODE.concat(Constants.INPUT_GRAPH),          false,          env)        .print();    }  }  @Override  public String getDescription() {    return MappingAnalysisExample.class.getName();  }}